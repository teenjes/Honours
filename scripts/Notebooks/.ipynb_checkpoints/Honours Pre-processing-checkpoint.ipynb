{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "\n",
    "_nsre = re.compile('([0-9]+)')\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(_nsre, s)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_extended.fastq\", \"fastq\"))\n",
    "SeqIO.write(fastq_dict.values(), \"../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_extended.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_dict1 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_for_use.fasta\", \"fasta\"))\n",
    "fasta_dict2 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_extended.fasta\", \"fasta\"))\n",
    "print(len(fasta_dict1))\n",
    "print(len(fasta_dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthe = []\n",
    "for key in fasta_dict2:\n",
    "    lengthe.append(len(fasta_dict2[key].seq))\n",
    "print(min(lengthe))\n",
    "print(max(lengthe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {**fasta_dict1,**fasta_dict2}\n",
    "print(len(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqIO.write(combined.values(), \"../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_for_use.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_for_use.fasta\", \"fasta\"))\n",
    "print(len(combined_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastq_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171103_FAH15473/barcode02/merged.fastq\", \"fastq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for key in fastq_dict:\n",
    "    lengths.append(len(fastq_dict[key].seq))\n",
    "print(\"The number of reads in this file is\", len(fastq_dict))\n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "\n",
    "\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Reads spread 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA = fastq_dict.copy()\n",
    "for key in fastq_dict:\n",
    "    if len(fastq_dict[key].seq) not in range(2700, 3200):\n",
    "        del frDNA[key]\n",
    "print(\"The number of reads between 2700 and 3200 bp in length is\", len(frDNA))\n",
    "EF1a = fastq_dict.copy()\n",
    "for key in fastq_dict:\n",
    "    if len(fastq_dict[key].seq) not in range(900, 1400):\n",
    "        del EF1a[key]\n",
    "print(\"The number of reads between 900 and 1400 bp in length is\", len(EF1a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the reads defined between the frDNA cutoff into a new fasta file\n",
    "SeqIO.write(frDNA.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_clipped.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the reads defined between the EF1a cutoff into a new fasta file\n",
    "SeqIO.write(EF1a.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_EF1a_clipped.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_extract={k: frDNA[k] for k in list(frDNA.keys())[:500]}\n",
    "EF1a_extract={k: EF1a[k] for k in list(EF1a.keys())[:500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in frDNA_extract:\n",
    "    frDNA_extract[key].annotations = 'frDNA'\n",
    "for key in EF1a_extract:\n",
    "    EF1a_extract[key].annotations = 'EF1a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_extract = {}\n",
    "combined_extract.update(frDNA_extract)\n",
    "combined_extract.update(EF1a_extract)\n",
    "print(len(frDNA_extract))\n",
    "print(len(EF1a_extract))\n",
    "print(len(combined_extract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SeqIO.write(frDNA_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_extract_test.fastq\", \"fastq\")\n",
    "SeqIO.write(EF1a_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_EF1a_extract_test.fastq\", \"fastq\")\n",
    "SeqIO.write(combined_extract.values(), \"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_combined_extract_test.fastq\", \"fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/frDNA_clipped_test.paf\", sep='\\t', header=None, engine='python')\n",
    "EF1a_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/EF1a_clipped_test.paf\", sep='\\t', header=None, engine='python')\n",
    "combined_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/combined_test.paf\", sep='\\t', header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_paf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min len of match for frDNA is\", frDNA_paf[1].min())\n",
    "print(\"min len of match for EF1a is\", EF1a_paf[1].min())\n",
    "print(\"min len of match for combined is\", combined_paf[1].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num matches with unique ids for frDNA is', len(frDNA_paf[0].unique()))\n",
    "print('num matches with unique ids for EF1a is', len(EF1a_paf[0].unique()))\n",
    "print('num matches with unique ids for combined is', len(combined_paf[0].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare above (using minimap2) with BLAST approach\n",
    "    - BLAST may be too slow on a larger dataset\n",
    "\n",
    "\n",
    "Check with other alignment programs eg. lastz, BLAT (check for others)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of full size-clipped files for alignment via minimap2\n",
    " - For each of the frDNA_clipped and EF1a_clipped files as created above \n",
    "     - Look for the number of unique ids in the resultant file\n",
    "     - Determine the percentage of reads in this range that match homology given total number of reads in the clipped.fastq file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frDNA_clipped_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/frDNA_clipped_test.paf\", sep='\\t', header=None, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EF1a_clipped_paf = pd.read_csv(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/EF1a_clipped_test.paf\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for frDNA is', len(frDNA_clipped_paf[0].unique()))\n",
    "print('Percentage of matches in region =', \"{:.3%}\".format((len(frDNA_clipped_paf[0].unique())/167054)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for EF1a is', len(EF1a_clipped_paf[0].unique()))\n",
    "print('False positive percentage =', \"{:.3%}\".format((len(EF1a_clipped_paf[0].unique())/192712)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of matches found for the total of reads is', len(combined_paf[0].unique()))\n",
    "print('Percentage of matches overall =', \"{:.3%}\".format((len(combined_paf[0].unique())/413127)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Plot distribution of matching reads as above~~\n",
    "\n",
    "~~Repeat minimap2 for above to see if fluctuations~~\n",
    "\n",
    "Explore non-mapping 25%\n",
    "\n",
    "Scaling - plots saved out, file for statistics (loop over later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_ids = []\n",
    "# for key in combined_paf[0].unique():\n",
    "#     combined_ids.append(key)\n",
    "combined_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171103_FAH15473/barcode02/merged.fastq\", \"fastq\"))\n",
    "comb_dict = {}\n",
    "for key in combined_paf[0].unique():\n",
    "    comb_dict[key] = combined_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "comb_keys = []\n",
    "for key in comb_dict:\n",
    "    lengths.append(len(comb_dict[key].seq))\n",
    "    comb_keys.append(key)\n",
    "\n",
    "mean = np.mean(lengths)\n",
    "std = np.std(lengths)\n",
    "print(mean)\n",
    "print(std)\n",
    "    \n",
    "# stats_dict = {'number of frDNA reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "# stats = pd.DataFrame(stats_dict, index=['20171103_FAH15473/barcode02'])\n",
    "        \n",
    "              \n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "# ax.set(xlim=(250, 3500))\n",
    "# ax.set_title(\"frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False, bins=5000)\n",
    "# ax.set(xlim=(2400, 3500))\n",
    "# ax.set_title(\"frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Python_Processing/20171103_FAH15473/barcode02/barcode02_frDNA_clipped.fastq\", 'fastq'))\n",
    "print(len(fr_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_dict = {}\n",
    "# for key in fr_dict:\n",
    "#     if key not in frDNA_clipped_paf[0].unique():\n",
    "#         non_dict[key] = fr_dict[key]\n",
    "# print(len(non_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = []\n",
    "# non_keys = []\n",
    "# for key in non_dict:\n",
    "#     lengths.append(len(non_dict[key].seq))\n",
    "#     non_keys.append(key)\n",
    "\n",
    "# non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "# stats = pd.DataFrame(non_dict_stats, index=['20171103_FAH15473/barcode02'])\n",
    "        \n",
    "              \n",
    "# ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "# # ax.set(xlim=(250, 3500))\n",
    "# ax.set_title(\"non-frDNA reads for 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "# ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# plt.show()\n",
    "# display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../summary_statistics.py\n",
    "\n",
    "\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\"\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"full_file\", help=\"The full, unfiltered file containing all reads for this barcode\")\n",
    "parser.add_argument(\"input_folder\", help=\"The destination folder within which the .paf files are generated\")\n",
    "parser.add_argument(\"output_folder\", help=\"The destination folder for any outputs from this script - including summary statistics file and plots\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('\\033[0;35m'+'START'+'\\033[1;37m')\n",
    "\n",
    "output_folder = args.output_folder.rsplit('/', 1)[-2]\n",
    "input_folder = args.input_folder.rsplit('/', 1)[-2]\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input folder is \" + input_folder + '\\033[1;37m')\n",
    "    print('\\033[0;31m' + \"Output folder is \" + output_folder + '\\033[1;37m')\n",
    "    print('\\033[0;34m' + \"Loading \" + args.full_file + '\\033[1;37m')\n",
    "\n",
    "# Load the full file containing all reads for this barcode\n",
    "full_file_dict = SeqIO.to_dict(SeqIO.parse(args.full_file, \"fastq\"))\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + args.full_file + '\\033[1;37m')\n",
    "\n",
    "# Extract the information about the lengths of the sequence for each read in this barcode\n",
    "full_lengths = []\n",
    "for key in full_file_dict:\n",
    "    full_lengths.append(len(full_file_dict[key].seq))\n",
    "full_lengths_len = len(full_file_dict)\n",
    "\n",
    "\n",
    "# Plot the spread of read lengths for this barcode\n",
    "    # Expect to see two peaks - one for EF1a and one for frDNA\n",
    "ax = sns.distplot(full_lengths, color=\"k\", kde=False, bins=5000)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Read spread for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure1 = ax.get_figure()\n",
    "# Save this figure out\n",
    "figure1.savefig('/'.join([output_folder, 'full_read_spread.png']))\n",
    "figure1.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Full spread image file saved to \" + '/'.join([output_folder, 'full_read_spread.png']) + '\\033[1;37m')\n",
    "    print('\\033[0;34m' + \"Loading \" + input_folder+\"/combined_test.paf\" + '\\033[1;37m')\n",
    "\n",
    "# Import the PAF file resulting from the minimap2 homology filtering\n",
    "full_paf = pd.read_csv(input_folder+\"/combined_test.paf\", sep='\\t', header=None, engine='python')\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + input_folder+\"/combined_test.paf\" + '\\033[1;37m')\n",
    "# Determine all the read ids present within the homology-filtered dataset\n",
    "# Then, create a dictionary extracting all the information from the full read file, but ONLY for reads present within the homology-filtered data\n",
    "full_dict = {}\n",
    "for key in full_paf[0].unique():\n",
    "    full_dict[key] = full_file_dict[key]\n",
    "\n",
    "# For each key in the homology-filtered dictionary, extract the sequence length and key\n",
    "full_paf_lengths = []\n",
    "full_keys = []\n",
    "for key in full_dict:\n",
    "    full_paf_lengths.append(len(full_dict[key].seq))\n",
    "    full_keys.append(key)\n",
    "\n",
    "mean = np.mean(full_paf_lengths)\n",
    "std = np.std(full_paf_lengths)\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[1;33m' + 'Mean read length is %s' % mean + '\\033[1;37m')\n",
    "    print('\\033[1;33m' + 'Standard deviation of read length is %s' % std + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "length_filt_dict = full_dict.copy()\n",
    "for key in full_keys:\n",
    "    if len(full_dict[key].seq) < (mean-1.645*std) or len(full_dict[key].seq) > (mean+1.645*std):\n",
    "        del length_filt_dict[key]\n",
    "\n",
    "        \n",
    "        \n",
    "SeqIO.write(length_filt_dict.values(), '/'.join([output_folder, 'length_restricted_reads.fasta']), \"fasta\")\n",
    "if args.verbose:\n",
    "    print('\\033[1;36m' + 'Saved %s' % ('/'.join([output_folder, 'length_restricted_reads.fasta'])) + '\\033[1;37m')     \n",
    "    \n",
    "    \n",
    "    \n",
    "length_filt_lens = []\n",
    "len_filt_keys = []\n",
    "for key in length_filt_dict:\n",
    "    length_filt_lens.append(len(length_filt_dict[key].seq))\n",
    "    len_filt_keys.append(key)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "Extract the qscores\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loading \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "summ_stats_csv = pd.read_csv('Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt', sep='\\t', header=None, names=['filename', 'read_id', 'run_id', 'batch_id', 'channel', 'mux', 'start_time', 'duration', 'num_events', 'passes_filtering', 'template_start', 'num_events_template', 'template_duration', 'sequence_length_template', 'mean_qscore_template', 'strand_score_template', 'median_template', 'mad_template'], engine='python')\n",
    "summ_stats_csv = pd.DataFrame(summ_stats_csv[1:])\n",
    "summary_list = []\n",
    "for column, row in summ_stats_csv.iterrows():\n",
    "    if row['read_id'] in full_keys:\n",
    "        summary_list.append([row['read_id'], row['mean_qscore_template']])\n",
    "summary_frame = pd.DataFrame(summary_list)\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Finished with \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "    \n",
    "Create a dictionary containing the statistics for the filtered dataset\n",
    "    Total no. frDNA reads, Min. read length, Max. read length, Mean read length, Median read length, Quality score\n",
    "\n",
    "stats_dict = {'number of frDNA reads':len(length_filt_lens),'minimum read length':min(length_filt_lens),'maximum read length':max(length_filt_lens),'mean read length':\"{:.0f}\".format(np.mean(length_filt_lens)),'std dev':\"{:.0f}\".format(np.std(length_filt_lens)),'median read length':\"{:.0f}\".format(np.median(length_filt_lens))\n",
    "              ,'min_qscore':\"{:.2f}\".format(min(summary_frame[1].astype(float))), 'max_qscore':\"{:.2f}\".format(max(summary_frame[1].astype(float))), 'mean_qscore':\"{:.2f}\".format(np.mean(summary_frame[1].astype(float))), 'median_qscore':\"{:.2f}\".format(np.median(summary_frame[1].astype(float)))\n",
    "             }\n",
    "stats = pd.DataFrame(stats_dict, index=['%s' % '/'.join(args.full_file.rsplit('/')[-3:-1])])    \n",
    "              \n",
    "bx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "bx.set(xlim=(250, 3500))\n",
    "bx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "bx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "bx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure2 = bx.get_figure()\n",
    "figure2.savefig('/'.join([output_folder, 'frDNA_len_filt_full.png']))\n",
    "figure2.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_full.png']) + '\\033[1;37m')\n",
    "\n",
    "cx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "cx.set(xlim=((mean-1.645*std)-100, (mean+1.645*std)+100))\n",
    "cx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "cx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "cx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure3 = cx.get_figure()\n",
    "figure3.savefig('/'.join([output_folder, 'frDNA_len_filt_limited.png']))\n",
    "figure3.clf()\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Zoomed-in frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_limited.png']) + '\\033[1;37m')\n",
    "\n",
    "stats.to_csv('/'.join([output_folder, 'frDNA_len_filt_statistics.csv']), index=False)\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Summary statistics file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_statistics.csv']) + '\\033[1;37m')\n",
    "    \n",
    "print('\\033[0;35m'+'END'+'\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/media/MassStorage/tmp/TE/honours/analysis/Stats/*/*/frDNA_statistics.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','standard deviation','median read length','min_qscore','max_qscore','mean_qscore','median_qscore'])\n",
    "for path in path_names:\n",
    "    if path[67:-21] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            print(path_stats_csv)\n",
    "            full_stats = full_stats.append(path_stats_csv)\n",
    "            full_stats = full_stats.rename(index={0: path[49:-21]})\n",
    "full_stats = full_stats.sort_index(ascending=True)\n",
    "full_stats.to_csv('../../analysis/Stats/overall_frDNA_stats.csv')\n",
    "num_read_stats = pd.DataFrame(data=[\"{:.0f}\".format(min(full_stats['number of frDNA reads'])), \"{:.0f}\".format(max(full_stats['number of frDNA reads'])), \"{:.0f}\".format(np.mean(full_stats['number of frDNA reads'])), \"{:.0f}\".format(np.median(full_stats['number of frDNA reads']))], index=['Min', 'Max', 'Mean', 'Median'], columns=['Number of reads'])\n",
    "num_read_stats.to_csv('../../analysis/Stats/number_of_frDNA_reads_summary.csv')\n",
    "full_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(full_stats['number of frDNA reads']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','standard deviation','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_index(ascending=True)\n",
    "display(full_stats2)\n",
    "full_stats2.to_csv('../../analysis/Length_Filtered/overall_frDNA_stats.csv')\n",
    "num_read_stats = pd.DataFrame(data=[\"{:.0f}\".format(min(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(max(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(np.mean(full_stats2['number of frDNA reads'])), \"{:.0f}\".format(np.median(full_stats2['number of frDNA reads']))], index=['Min', 'Max', 'Mean', 'Median'], columns=['Number of reads'])\n",
    "num_read_stats.to_csv('../../analysis/Length_Filtered/number_of_frDNA_reads_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create matrix for loss-of-reads when performing length filtering as percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stats_1 = []\n",
    "full_stats_2 = []\n",
    "result = []\n",
    "\n",
    "for i in full_stats['number of frDNA reads']:\n",
    "    full_stats_1.append(i)\n",
    "for i in full_stats2['number of frDNA reads']:\n",
    "    full_stats_2.append(i)\n",
    "for i in range (0,len(full_stats_1)):\n",
    "    result.append(float(\"{:.2f}\".format((full_stats_1[i]-full_stats_2[i])/full_stats_1[i])))\n",
    "print(result)\n",
    "print('The maximum loss of reads is %s%% in %s' % (100*max(result), path_names[result.index(max(result))][46:-30]))\n",
    "print('The minimum loss of reads is %s%% in %s' % (100*min(result), path_names[result.index(min(result))][46:-30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAH18688_barcode10 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171212_FAH18688/barcode10/merged.fastq\", 'fastq'))\n",
    "lengths = []\n",
    "b10_keys = []\n",
    "for key in FAH18688_barcode10:\n",
    "    lengths.append(len(FAH18688_barcode10[key].seq))\n",
    "    b10_keys.append(key)\n",
    "\n",
    "non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "stats = pd.DataFrame(non_dict_stats, index=['20171212_FAH18688/barcode10'])\n",
    "              \n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "# ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Miscalled read spread for 20171212_FAH18688/barcode10\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "plt.show()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAH18654_barcode10 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Concatenated/20171207_FAH18654/barcode10/merged.fastq\", 'fastq'))\n",
    "lengths = []\n",
    "b10_keys = []\n",
    "for key in FAH18654_barcode10:\n",
    "    lengths.append(len(FAH18654_barcode10[key].seq))\n",
    "    b10_keys.append(key)\n",
    "\n",
    "non_dict_stats = {'number of reads':len(lengths),'minimum read length':min(lengths),'maximum read length':max(lengths),'mean read length':\"{:.0f}\".format(np.mean(lengths)),'median read length':\"{:.0f}\".format(np.median(lengths))}\n",
    "stats = pd.DataFrame(non_dict_stats, index=['20171207_FAH18654/barcode10'])\n",
    "              \n",
    "ax = sns.distplot(lengths, color=\"k\", kde=False)\n",
    "# ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Miscalled read spread for 20171207_FAH18654/barcode10\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "plt.show()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode02 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\", \"fasta\"))\n",
    "barcode06 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode06/length_restricted_reads.fasta\", \"fasta\"))\n",
    "\n",
    "total_lens = []\n",
    "for key in barcode02:\n",
    "    total_lens.append(len(barcode02[key].seq))\n",
    "for key in barcode06:\n",
    "    total_lens.append(len(barcode06[key].seq))\n",
    "print(max(total_lens))\n",
    "print(min(total_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode02_numbers = {}\n",
    "for key in barcode02:\n",
    "    seq = []\n",
    "    for element in barcode02[key].seq[30:-30]:\n",
    "        if element == \"A\":\n",
    "            seq.append(0)\n",
    "        elif element == \"C\":\n",
    "            seq.append(1)\n",
    "        elif element == \"G\":\n",
    "            seq.append(2)\n",
    "        elif element == \"T\":\n",
    "            seq.append(3)\n",
    "    if len(seq) < max(total_lens):\n",
    "        seq.extend([0]*(max(total_lens)-len(seq)))\n",
    "    barcode02_numbers[key] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2 = np.array(random.choices(list(barcode02_numbers.values()),k=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode06_numbers = {}\n",
    "for key in barcode06:\n",
    "    seq = []\n",
    "    for element in barcode06[key].seq[30:-30]:\n",
    "        if element == \"A\":\n",
    "            seq.append(0)\n",
    "        elif element == \"C\":\n",
    "            seq.append(1)\n",
    "        elif element == \"G\":\n",
    "            seq.append(2)\n",
    "        elif element == \"T\":\n",
    "            seq.append(3)\n",
    "    if len(seq) < max(total_lens):\n",
    "        seq.extend([0]*(max(total_lens)-len(seq)))\n",
    "    barcode06_numbers[key] = seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq6 = np.array(random.choices(list(barcode06_numbers.values()),k=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_comb = np.concatenate((seq2, seq6), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids2 = np.array([2]*(len(seq2)))\n",
    "print(len([2]*(len(seq2))))\n",
    "ids6 = np.array([6]*(len(seq6)))\n",
    "print(len([2]*(len(seq6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_comb = np.concatenate((ids2, ids6), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seq_comb))\n",
    "print(len(ids_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv', ids_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_comb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv', seq_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_test = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_seqs.csv.npz', allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test = np.load('../../analysis/arrays_test/20171103_FAH15473_b2+b6_ids.csv.npz', allow_pickle=True)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Concatenated/*/*/*.fastq\"\n",
    "path_names = glob.glob(path)\n",
    "total_count = 0\n",
    "can_count = 0\n",
    "mis_count = 0\n",
    "unc_count = 0\n",
    "for path in path_names:\n",
    "    temp_dict = SeqIO.to_dict(SeqIO.parse(path, \"fastq\"))\n",
    "    if path[61:-13] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            total_count += len(temp_dict)\n",
    "            can_count += len(temp_dict)\n",
    "        else:\n",
    "            total_count += len(temp_dict)\n",
    "            mis_count += len(temp_dict)\n",
    "            can_count += len(temp_dict)\n",
    "    else:\n",
    "        total_count += len(temp_dict)\n",
    "        unc_count += len(temp_dict)\n",
    "print('Total number of reads assigned to a barcode by Deepbinner is %s' % can_count)\n",
    "print('Number of reads assigned to non-existent barcodes by Deepbinner is %s' % mis_count)\n",
    "print('Estimated number of misassigned reads in total is %s' % (23*mis_count))\n",
    "print('Percentage of misassigned reads based on estimated number is %s' % (2300*(mis_count)/can_count))\n",
    "print('Number of unclassified reads is %s' % unc_count)\n",
    "print('Percentage of unclassified reads is %s' % (100*unc_count/total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','std dev','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_values('mean read length', ascending=False)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(10,17))\n",
    "ax = sns.barplot(x=full_stats2['mean read length'], y=full_stats2.index, palette=\"Blues_d\", xerr=full_stats2['std dev'],ci=True)\n",
    "ax.set_title(\"Mean Read Length by Sample\")\n",
    "figure1 = ax.get_figure()\n",
    "figure1.savefig(\"/10tb/tmp/TE/honours/analysis/Length_Filtered/mean_reads.png\",bbox_inches = \"tight\")\n",
    "figure1.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/10tb/tmp/TE/honours/analysis/Length_Filtered/*/*/*.csv\"\n",
    "path_names = glob.glob(path)\n",
    "full_stats2 = pd.DataFrame(data=None, columns = ['number of frDNA reads','minimum read length','maximum read length','mean read length','std dev','median read length'])\n",
    "for path in path_names:\n",
    "    if path[64:-30] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            path_stats_csv = pd.read_csv(path, header=0)\n",
    "            full_stats2 = full_stats2.append(path_stats_csv)\n",
    "            full_stats2 = full_stats2.rename(index={0: path[46:-30]})\n",
    "full_stats2 = full_stats2.sort_values('number of frDNA reads', ascending=False)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(10,17))\n",
    "ax = sns.barplot(x=full_stats2['number of frDNA reads'], y=full_stats2.index, palette=\"Blues_d\")\n",
    "ax.set_title(\"Number of Reads per Sample after Filtering\")\n",
    "figure1 = ax.get_figure()\n",
    "figure1.savefig(\"/10tb/tmp/TE/honours/analysis/Length_Filtered/num_reads.png\",bbox_inches = \"tight\")\n",
    "figure1.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify primers and orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mothur()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.pcr.seqs(fasta=\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\",oligos=\"../../analysis/ITS_primers.oligos\",pdiffs=0,rdiffs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.pcr.fasta\",\"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open('../../analysis/Length_Filtered/20171103_FAH15473/barcode02/ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_reads.fasta\",\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "keys_list = random.sample(ids,k=200)\n",
    "print(len(keys_list))\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "print(len(tmp_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../get_cons_ids.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to extract the read ids of reads that\n",
    "contain both the forward and reverse primer as exact matches\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "m = Mothur()\n",
    "m.pcr.seqs(fasta=args.input_file,oligos='ITS_primers.oligos',pdiffs=2,rdiffs=2)\n",
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(args.input_file[:-5]+\"pcr.fasta\",\"fasta\"))\n",
    "ids = []\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open(args.input_file[:-29]+'ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:-29]+'ids.txt') + '\\033[1;37m')\n",
    "    print('\\033[0;32m' + (\"The number of reads is %s\" % len(ids)) + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "if len(ids) > 500:\n",
    "    keys_list = random.sample(ids,k=500)\n",
    "else:\n",
    "    print('\\033[1;37m' + \"LOW READS\")\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),('Consensus'+args.input_file[15:-29]+'for_consensus_500.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + ('Consensus'+args.input_file[15:-29]+'for_consensus_500.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../get_cons_ids3.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to extract the read ids of reads that\n",
    "contain both the forward and reverse primer as exact matches\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "m = Mothur()\n",
    "m.pcr.seqs(fasta=args.input_file,oligos='ITS_primers.oligos',pdiffs=0,rdiffs=0)\n",
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(args.input_file[:-5]+\"pcr.fasta\",\"fasta\"))\n",
    "ids = []\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open(args.input_file[:-29]+'ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:-29]+'ids.txt') + '\\033[1;37m')\n",
    "    print('\\033[0;32m' + (\"The number of reads is %s\" % len(ids)) + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "if len(ids) > 100:\n",
    "    keys_list = random.sample(ids,k=100)\n",
    "else:\n",
    "    print('\\033[1;37m' + \"LOW READS\")\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),('Consensus'+args.input_file[15:-29]+'for_consensus_100.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + ('Consensus'+args.input_file[15:-29]+'for_consensus_100.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../get_cons_ids2.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to extract the read ids of reads that\n",
    "contain both the forward and reverse primer as exact matches\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "m = Mothur()\n",
    "m.pcr.seqs(fasta=args.input_file,oligos='ITS_primers.oligos',pdiffs=2,rdiffs=2)\n",
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "ids = []\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open(args.input_file[:-29]+'ids2.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:-29]+'ids2.txt') + '\\033[1;37m')\n",
    "    print('\\033[0;32m' + (\"The number of reads is %s\" % len(ids)) + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "if len(ids) > 1000:\n",
    "    keys_list = random.sample(ids,k=1000)\n",
    "else:\n",
    "    print('\\033[1;37m' + \"LOW READS\")\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),('Consensus'+args.input_file[15:-29]+'for_consensus_1000.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + ('Consensus'+args.input_file[15:-29]+'for_consensus_1000.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../get_final_cons.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to extract the read ids of reads that\n",
    "contain both the forward and reverse primer as exact matches\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "m = Mothur()\n",
    "m.pcr.seqs(fasta=args.input_file,oligos='ITS_primers.oligos',pdiffs=2,rdiffs=2)\n",
    "pcr_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "ids = []\n",
    "for key in pcr_dict:\n",
    "    ids.append(key)\n",
    "with open(args.input_file[:-29]+'ids2.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)\n",
    "    \n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:-29]+'ids2.txt') + '\\033[1;37m')\n",
    "    print('\\033[0;32m' + (\"The number of reads is %s\" % len(ids)) + '\\033[1;37m')\n",
    "    \n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "if len(ids) > 100:\n",
    "    keys_list = random.sample(ids,k=100)\n",
    "else:\n",
    "    print('\\033[1;37m' + \"LOW READS\")\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),('Consensus'+args.input_file[15:-29]+'final_cons.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + ('Consensus'+args.input_file[15:-29]+'final_cons.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Align import AlignInfo\n",
    "alignment = Bio.AlignIO.read(\"../../analysis/Consensus/20171103_FAH15473/barcode02/consensus_100.fasta\",\"fasta\")\n",
    "summary_align = AlignInfo.SummaryInfo(alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus = summary_align.dumb_consensus(threshold=0.7, ambiguous='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(consensus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = SeqIO.to_dict(SeqIO.parse(\"../../database/sh_refs_qiime_ver8_dynamic_02.02.2019.fasta\", \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for key in database:\n",
    "    lens.append(len(database[key].seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(lens))\n",
    "print(min(lens))\n",
    "print(np.mean(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of reads in this file is\", len(lens))\n",
    "ax = sns.distplot(lens, color=\"k\", kde=False)\n",
    "\n",
    "\n",
    "# ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Reads spread 20171103_FAH15473/barcode02\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i > 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([1 for i in lens if i < 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../get_align_seqs.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "parser.add_argument(\"num_reads\", help=\"The number of reads to extract\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "    print('\\033[0;31m' + \"The number of extracted reads is \" + '\\033[0;32m' + args.num_reads + '\\033[1;37m')\n",
    "    \n",
    "tmp_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "new_dict = tmp_dict.copy()\n",
    "\n",
    "ids=[]\n",
    "for key in tmp_dict:\n",
    "    ids.append(key)\n",
    "    \n",
    "keys_list = random.sample(ids,k=int(args.num_reads))\n",
    "\n",
    "for key in new_dict:\n",
    "    if key not in keys_list:\n",
    "        del tmp_dict[key]\n",
    "SeqIO.write(tmp_dict.values(),(args.input_file[:9]+'Alignment'+args.input_file[24:-29]+ args.num_reads + '_reads.fasta'),'fasta')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Ids file saved to \" + '\\033[0;35m' + (args.input_file[:9]+'Alignment'+args.input_file[24:-29]+ args.num_reads + '_reads.fasta') + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss_test = \"\"\"nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnAGnnnnnnnnnnnnnGTTAACnACnAnnAA\n",
    "GACAnCnCGACnnnAACnnTTTCTTnCAGnnnCACCTGCAAGTCTGGTGCCAGCAGCCnG\n",
    "CnnnnnnGnnTAAnTnTCCAGCTCnnCAAnTAnnnGCnGTATAnTTnnnAAAGnTTGnnT\n",
    "TGAnnCGTTnnAAAAAGCnTCnnnGTnAGnnnTCGAnnAnCTTnnnnCGGCnnCTnCTGG\n",
    "CnnnnnAGTTnnGGTCnnnCGCnnnnCTnTTnTGnnGTGnTnGTAnnnnCTGnATTTGTT\n",
    "GnnnGAGnGCTTnnACCTnCTnnnnTnnGGTnGAACTTnCnnnAATGCACnTTTnnACnT\n",
    "GGGnnTGTTnGGAnnGGGAACCnnAGGATTTTnnnnnnnnTnnAAAAATTAnGnAGTGTT\n",
    "CnnAAAGCAnnGnnGCTTAnTGCCTnnnGAAnnTAnnnCATTAGCnATnnGGAAnnTnnn\n",
    "AATnnAAAATnnAGGACnGnnnnnnnTGnnnTGAnnnTTCTAnTTTnnnnTGnTTGGnTT\n",
    "TCTnnAGGATnnTACCnnnnnGTAnATGATnnnnGAATnnnnnAGGGTnnCAGnnTTGnn\n",
    "nnGGGnnnnGCAnnTTTnnGTAnnnnTnnTACATCnGTnCAGAGnnnGTnnGnAAATTnn\n",
    "CTTnnnnnGGnATTGATnGnnTAAGnnAnCAAACTACnnnTGCGAAnnnnnGCATCnTGC\n",
    "CAAnnGnGATGACTTnnCATTGATCnnAAGAACGnnAAGnGTTAAGnnnGnGTTCAAAAA\n",
    "nnCGATnnCnnnAGATnnAnnnCCnnGnnnTTGnTnnnnAGTnnCTTAACnnnAGTnnAA\n",
    "ACTnnAnnnTGCnnCGACnnTGnnGGGATCAGnACnnnAAGGATTTATnAnnnATGnnAC\n",
    "TTGTTTGnnGCACCCAAnnnnnnnnnGGAAACCTnnnGAAGnnnnTTTAnGGnTTCGTGG\n",
    "nnnnGGnnAGTACGGTnCACnnAAGnGCnnnnnnnTGAAACnTTnnAAAGGAATTGAnCG\n",
    "GAAnnnGGGCACCnnnACCAnnGGTnGTGnnGAGCnnnnCTGCannGCTTnAATTnnTGA\n",
    "nCTCAACACnnnnnGGGAAnACTnnnCAnCCAnGGnnnTCCAGAnnnnnnCACnAGTnnn\n",
    "AAGGATnTGAnnnCAnGAnnnnTTGnnATAnGCnnTTTTTnCTTnGAnTTTTGnnTGGnn\n",
    "TTGnnnnGTGnGTGCAnTGnnnnGCnnCGnnnTTCnnnnTTAnnGnTTGGTGnGnnAGTG\n",
    "nnATTnnTGTCnnnnTnGGnnTTAATTnCCGATAnnnACnnGAACnnnGAGAnCnnCTTC\n",
    "TCnnCTGnCTnnAAATAGTnnnCCAGCnnTGGCnnTACGnnGnnCnTGGCnTnnGCnTGA\n",
    "nnCnnTTCTTnAGnnAGGGACTnnATCAnnACGnTTnTAGTnnTGATGnGAnAGTTGGAn\n",
    "nnnGCAAnTAnnACnAGGTCTnnnGTGATGnnCCCTTnnAGATGnTTCnnTGnnnnnGGC\n",
    "nCGCAnCGCnnnnnnGCnTACAnnCTGACnnCnAAGCnnCAGCnnGAGnnnTAnTATCAC\n",
    "nnnCnTTATCTnnnnAAAAGATTGnnnGGTnnnAATCnTTnGTGAAAnCnnTTGnnGTCn\n",
    "nGTGAnTnnGGGGAnnTnnnAGAGCnnATTGCnAnnATTATTGCnnnnTCTTCAnACnnG\n",
    "AnnnGGAAnTACCnnnnTnAGTAnnAGnCGTATnnGTCnnATnCnnnAGCnATnnGCGnT\n",
    "TnnnnnnGATTnnACGnnTnCCCTGnCCnCTTTGnnnnTACAnnnnnCACCGnCCCGnnn\n",
    "TCGCTnnnnACTACCnGnnATTGGCnnnAGGCnnnnnTTTTTnnGAnnnGGCGTTnnCGG\n",
    "AGAGTnCTAnTAAGnGAnnnnGCTnGnnGCAACnAnGCAnnTnnCTTACTnnnGATTCAA\n",
    "AGnnnnTTCTnACGAnnAAATnnnGGnnnTCTGCTnnAGAGGAAnnnGTAAAnnnnAGTC\n",
    "GTnAACnnnnnAAGGTTTnCCGTnAnnnnGGTGAnACCnTGCnnGGAAnnnGGATCAnTT\n",
    "ATTnnnnAAAAGAACTAnGAGnnnnnnTGCACnTTnnnnTATTGnnnnnnnnnnTGGCTn\n",
    "CGACCCCnnTTTTnAAAAnnnATCTCAnnnCCCnnAAACnTTTTnnAAGACTTGGnnnTT\n",
    "GCATnnGATTTnGAAAGAnnnnnATnnnnCATnTGnnCAATTnnGnAGTnAGnnnACnnG\n",
    "TnnnnAACTTCnnTnTTATTnGAATGnTnnTGCATTACnCCTCCCnnnnnnnnnnnTTTT\n",
    "TTTTTATTAAAAATTnnACACAAAACnACnnAAGTTTAAAnnTnnnGAnATGTAAnnnCC\n",
    "AAACnnCnTTTAATnnnTATnAAATAnnACnnTTTTAACAnnnnATGnGATCnnTCTnnA\n",
    "GnnGCTCTnnnCACATnnCGATnnnGAnAGAACnnACAGnnTnGAAnnATGnnnTnGATn\n",
    "nnAAGnTAATGnnnTnGAATnnnnnnTGCnnAnnnGAATnnnnnnTCnAnnGTGAnnnnA\n",
    "TnCnnATCnGnnAATCTTTnnnnGAACGCnnACCnTTGCGCnnCTTTTnnnGGTAnTTCC\n",
    "nnnAAAGGCAnnCnACCTGTTnnnnnnnGTGTCAnnTGAAAnnCCnnnnnCTnCTCATnn\n",
    "nnnTnAAATAATTnnTTGnATTnnAATTnATnnTTTCnnnnAAnTGGATnGTTGnnAnGT\n",
    "GCnnnnnnnTGnCTGTnnnnAATTnnAGnCTCACnnTTTAAnnnATATATAnAGnnnnTn\n",
    "CACnnTTTTCTAnTAnnnnAGTTGGnAnnnTTGnnACTTGGnnnTGTAATnAAnTTTTAT\n",
    "CnATCnACAnnTCnnAAGnnnnGATnTGnnTAnGCAnnnnnATAnnnCTGCnnnCAnTCT\n",
    "nnnTATTTnnnnAAGGAGACTCCnnnAAAAACCCnnAATTTTnnAAnnnnnnCCTnnnTA\n",
    "AGACnnCTnnCAAATnCnAGGTnGnnGGACTAnCCCnnGCTnnnnGAnnACTTnnnAAGn\n",
    "nCATnATnCAATnnAAnnnGCnnGGAGGnnnAAAAGAAACnnTAACnnnAAGGAnnnnnn\n",
    "nnnTTCCnnnnCTAGnTAACGGnnCnnnnAGTnnGAAGAGGnnGAAnnAAGCCCnAAATT\n",
    "nnnTGTAnnnATnCnnnTGGCnnTCTTTnCnAGAGTCCnnnAGTTGTnnnnAAnnTTTTG\n",
    "nnnnAGnnAACTGnnnnnTTTTCnnAGTnnnnGCnTGGnnnACCAnnTGnnTAnnnnnTA\n",
    "AGnTCnnnTGnTTGAAAnnAGCnnAGCAnnnnnnnTnnCATTGAGGnnnnGnnnATAATC\n",
    "CnCGTTTnnnnAnTGATAnnTGnGACTACnnCAGnnnTGCnnATTAnnnnnnnTGAnnnT\n",
    "AnCnAnnnnnGTCTnnnCTAAnnGAGTnnnCnnnGAGTTGTTTnnGnnGGAATGnnnCAn\n",
    "GCTCnnAAAGnnnnTGGnGTGGnnnnnTAnnnAATTCnnCATnCTnnAAnnnGGnnnnnn\n",
    "nCTAnnnAAnTATAnnnGGTGnAnGAGACCnGAnnnTAGCnnAAnnACAAGnTnnnnACn\n",
    "CGTGAnnGGGAAAnnnnATGAAAnnnAGAACTTTGGAAnnAGAGnnnnAGTTAACnAGnn\n",
    "nnTACnGTnnnnGAAAnnnnTTGnnTTAAAnnnnnGAAACnnAnnnCTTGnnAAGnTTnn\n",
    "nAnGACTnTGTTATTnnATTAGnTTCAAnnCnnCTTTTTGACnGnnAGGnnAGnnnnTAT\n",
    "TnCTnnnAATnnGATTAnnACnAGAnCCAAnnnnnCATCnAAnTTTnnTTGGnGnnnTGn\n",
    "TTGGAGAAGnGGnnnnTTTAAGnGAAATGnnTAGCnnAnGTCTnCnnTGAnnnnCnnnTG\n",
    "TGTnnnTATnnnnAGnTCnCTGAGCnTTTGAnTnnACAATnGCTTnnnnnAAGATTGAnn\n",
    "GGAnAGGCnAGTAnnnAGCGCnnnAATnTTATnnTGTnGTGGAnnnnACAnnAATTnAAT\n",
    "GTTCnTnnnnTCTTACnnTnnnGAnGGnnnATGnnTTGGTnGTnAATAGCnnTnTTAAAT\n",
    "nnGAnCCnnnCGTnnnnnnnnCTTnnGnAAACnnnACnnGGACnnCAAGGAnGTnnCTAn\n",
    "ACATnGCTTGnCAnnnnnAGTATTTnGGGTGnnnnCTTnnnnnGAAACCnnCTTATGnnC\n",
    "GnnnnTAATGAAnnnAGTnnAAATnGTnnnAAATnnGGGAnnTCTGTTAAnnnAAGTGCA\n",
    "CCAnnnnTTGAnnCCnAGTCCnnnnAGATTATnnTnnTAnTATGATGGnnnnnTACTGnA\n",
    "GTnnAAGAGnnnnCAAnGnnTATGnnnTTGnnGGACCCGAnnAAGAnnnTGnGTnnGAnA\n",
    "CnnnTAnnTGnCCTGnnAAnnnTnAGGnGTnnnnGAAGCCnnAnGAGGAAACTnnnnnCn\n",
    "nnTGGTnnnnnGGAnnAGCTnnCGnTAGCnnnGTnnnnnTCTGAnnnnnCGnTGnCAnnn\n",
    "AATCnGATnCnnGTCnnAnnAATTTGGnnnnnGTATAGGGnnGCGAnnnAnnAGACTnAA\n",
    "TCnGnnnnnAnnnnCCATnnCnnnTAGTnAGCnnTGGnnnnTTCnnCTGCCnnnGAAGnT\n",
    "TTnnnnCCnCTCAGGATAnnnnnnGCAAAnnnnnGACTCnnGTATnCAGnTTTnTATnnn\n",
    "GAGGnTAAAGnnnnCGAnAnnnTGATTnnAGAGGCCTTGGGGnnATGAnAnnACnnnnAT\n",
    "CnnCTTAACCTnnnATTnnnnCTCnAAACnnTTTAAATnnnATGTnnAAGACnnnGCTnC\n",
    "nnCTGTTTCTTnnAATnnnnnTGAnnAnnnnCnnGTGGnnGCATnGnTGnnnnnAATnnn\n",
    "GAGAnGTCTTTnnAGTnnnGGGCCnAnTTTTTnnGGTAAGCAGAACTGnnGCGAnGGTGC\n",
    "nnTGnAAGAAnAGTTGTnCnnnGGTGTnnnnnCTTnnTGnnnnnnTGTTAACnCTnTAnn\n",
    "GCAAnnnTACnnnnGTAACnnnnnnnnnnnnn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emboss_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run loop over EMBOSS cons -identity option from 0 to 100 to get a range of different consensus sequences [x]\n",
    "##### Compare each consensus with the GENEIOUS consensus and with ~20 randomly selected reads (same reads for each consensus)\n",
    "###### Extract the score and determine what identity gives the best mean match score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test1.fasta\",\"fasta\"))\n",
    "emboss_test = str(test_dict['test'].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "scores_matrix = []\n",
    "test100 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Alignment/20171103_FAH15473/barcode02/100_reads.fasta\",\"fasta\"))\n",
    "for key in test100:\n",
    "    tmp = []\n",
    "    alignments = pairwise2.align.globalxx(emboss_new, test100[key].seq, score_only=True)\n",
    "    tmp.append(alignments)\n",
    "    scores_matrix.append(int(alignments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../consensus_test.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "print(emboss_new)\n",
    "\n",
    "scores_matrix = []\n",
    "test100 = SeqIO.to_dict(SeqIO.parse(\"analysis/Alignment/20171103_FAH15473/barcode02/100_reads.fasta\",\"fasta\"))\n",
    "for key in test100:\n",
    "    alignments = pairwise2.align.globalxx(emboss_new, test100[key].seq, score_only=True)\n",
    "    scores_matrix.append(int(alignments))\n",
    "\n",
    "print(scores_matrix)\n",
    "mean = np.mean(scores_matrix)\n",
    "median=np.median(scores_matrix)\n",
    "f=open((args.input_file[:47]+\"scores.txt\"),\"a+\")\n",
    "f.write(\">%s\\t%s\\t%s\\t\\n\" % (keys[0], mean, median))\n",
    "\n",
    "tmp = pd.DataFrame(scores_matrix)\n",
    "tmp.to_csv(args.input_file[:47]+\"%s.csv\" % keys[0],index=False,header=False)\n",
    "\n",
    "\n",
    "with open((args.input_file[:47]+\"%s_new.fasta\" % keys[0]),\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] +emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.input_file[:-29]+'ids.txt','w') as handle:\n",
    "    handle.writelines(\"%s\\n\" % name for name in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"../../analysis/Consensus/20171103_FAH15473/barcode02/scores.txt\",\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=contents.replace(\"\\t\",\",\").replace(\"\\n\",\"\").replace(\",>\",\">\").split(\">\")[1:]\n",
    "print(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "indices=[]\n",
    "for item in tmp:\n",
    "    indices.append(int(item.split(\",\")[0][4:]))\n",
    "    data.append(((float(item.split(\",\")[1])),float(item.split(\",\")[2])))\n",
    "frame = pd.DataFrame(index=indices,data=data,columns=[\"Mean Score\",\"Median Score\"])\n",
    "print(frame.loc[frame['Mean Score'].idxmax()].name)\n",
    "print(len(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frame.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(frame)\n",
    "    \n",
    "#     # Index 0 is the Geneious-generated Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "geneious = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test0.csv\",header=None)\n",
    "test20 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test20.csv\",header=None)\n",
    "test1 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test1.csv\",header=None)\n",
    "test52 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test52.csv\",header=None)\n",
    "test75 = pd.read_csv(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test75.csv\",header=None)\n",
    "\n",
    "print(ranksums(geneious,test1))\n",
    "print(ranksums(geneious,test20))\n",
    "print(ranksums(geneious,test52))\n",
    "print(ranksums(geneious,test75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pd.DataFrame(columns=['geneious','test1','test20','test52','test75'])\n",
    "plot['geneious'] = geneious[0]\n",
    "plot['test1'] = test1[0]\n",
    "plot['test20'] = test20[0]\n",
    "plot['test52'] = test52[0]\n",
    "plot['test75'] = test75[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(data=plot)\n",
    "ax.set_title(\"Different consensus sequence generation methos for 20171103_FAH15473/barcode02\", fontsize=12)\n",
    "ax.set_xlabel(\"Consensus Generation Method\", fontsize=10)\n",
    "ax.set_ylabel(\"Score\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test100 = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Alignment/20171103_FAH15473/barcode02/100_reads.fasta\",\"fasta\"))\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test0_new.fasta\",\"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_matrix = []\n",
    "tmp = []\n",
    "for key in test100:\n",
    "    alignments = pairwise2.align.globalxx(emboss_new, test100[key].seq)\n",
    "    scores_matrix.append(alignments)\n",
    "    tmp.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import paf\n",
    "2. Extract columns 10/11 for each row\n",
    "3. Save the alignment percentage identity to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test0.paf\",\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()  \n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = [int(line.split(\",\")[9]),int(line.split(\",\")[10])]\n",
    "geneious = pd.DataFrame.from_dict(tmp_dict,orient='index',columns=['matching bases','total bases'])\n",
    "geneious['alignment identity'] = 100*(geneious['matching bases']/geneious['total bases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"../../analysis/Consensus/20171103_FAH15473/barcode02/test20.paf\",\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()  \n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = [int(line.split(\",\")[9]),int(line.split(\",\")[10])]\n",
    "test20 = pd.DataFrame.from_dict(tmp_dict,orient='index',columns=['matching bases','total bases'])\n",
    "test20['alignment identity'] = 100*(test20['matching bases']/test20['total bases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = pd.DataFrame(columns=['geneious','test20'])\n",
    "plot['geneious'] = geneious['alignment identity']\n",
    "plot['test20'] = test20['alignment identity']\n",
    "\n",
    "ax = sns.violinplot(data=plot)\n",
    "ax.set_title(\"Different consensus sequence generation methods for 20171103_FAH15473/barcode02\", fontsize=12)\n",
    "ax.set_xlabel(\"Consensus Generation Method\", fontsize=10)\n",
    "ax.set_ylabel(\"Alignment Percentage Identity\", fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../cleanup.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program cleans a consensus sequence by removing any\n",
    "'n' or 'N' characters\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "with open(args.input_file[:47]+\"clean_consensus_1000.fasta\",\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../cleanup_final.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program cleans a consensus sequence by removing any\n",
    "'n' or 'N' characters\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "with open(args.input_file[:47]+\"final_clean_consensus.fasta\",\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../qscore_cleanup.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program cleans a consensus sequence by removing any\n",
    "'n' or 'N' characters\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "with open(args.input_file[:47]+\"clean_qscore_consensus.fasta\",\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../cleanup_100.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program cleans a consensus sequence by removing any\n",
    "'n' or 'N' characters\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "with open(args.input_file[:47]+\"clean_consensus_100.fasta\",\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../cleanup_101.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program cleans a consensus sequence by removing any\n",
    "'n' or 'N' characters\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "with open(args.input_file[:47]+\"clean_consensus_101.fasta\",\"w+\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to generate consensus\n",
    "\n",
    "    1. Subsample a set number of reads using the get_cons_ids python script line\n",
    "    2. From these reads, generate an alignment file using muscle via the create_consensus script line, on the Consensus/ folder\n",
    "    3. Use the emboss_cons script line to create a consensus from the alignment file, from the analysis folder\n",
    "        3.5 This also involves a cleanup phase where any 'n' or 'N' character is removed from the resulting file\n",
    "    4. Collect all cleaned consensus files in a single fasta file using the make_database.sh script\n",
    "    5. Rename each file???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../make_database_1000.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program writes a cleaned consensus to a database file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "    \n",
    "with open(\"database/custom_database_1000.fasta\",\"a\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../make_database_final.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program writes a cleaned consensus to a database file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.input_file != 'analysis/Consensus/20171207_FAH18654/barcode10/clean_qscore_consensus.fasta' and args.input_file != 'analysis/Consensus/20171212_FAH18688/barcode10/clean_qscore_consensus.fasta':\n",
    "    if args.verbose:\n",
    "        print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "    keys = []\n",
    "    test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "    for key in test_dict:\n",
    "        keys.append(key)\n",
    "    emboss_test = str(test_dict[keys[0]].seq)\n",
    "    emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "    with open(\"database/make_database_final.fasta\",\"a\") as handle:\n",
    "        handle.write(\">%s\\n\" % keys[0] + emboss_new + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../make_qscore_database.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program writes a cleaned consensus to a database file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.input_file != 'analysis/Consensus/20171207_FAH18654/barcode10/clean_qscore_consensus.fasta' and args.input_file != 'analysis/Consensus/20171212_FAH18688/barcode10/clean_qscore_consensus.fasta':\n",
    "    if args.verbose:\n",
    "        print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "    keys = []\n",
    "    test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "    for key in test_dict:\n",
    "        keys.append(key)\n",
    "    emboss_test = str(test_dict[keys[0]].seq)\n",
    "    emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "\n",
    "    with open(\"database/qscore_database.fasta\",\"a\") as handle:\n",
    "        handle.write(\">%s\\n\" % keys[0] + emboss_new + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../make_database_100.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program writes a cleaned consensus to a database file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "    \n",
    "with open(\"database/custom_database_100.fasta\",\"a\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../make_database_101.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program writes a cleaned consensus to a database file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "keys = []\n",
    "test_dict = SeqIO.to_dict(SeqIO.parse(args.input_file,\"fasta\"))\n",
    "for key in test_dict:\n",
    "    keys.append(key)\n",
    "emboss_test = str(test_dict[keys[0]].seq)\n",
    "emboss_new = emboss_test.replace('N','').replace('n','').replace('\\n','')\n",
    "    \n",
    "with open(\"database/custom_database_101.fasta\",\"a\") as handle:\n",
    "    handle.write(\">%s\\n\" % keys[0] + emboss_new + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../1000_minimap_result.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program extracts a specified number of reads from a fasta\n",
    "file and saves them to a new fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "f=open(args.input_file,\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()\n",
    "\n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = str(line.split(\",\")[5])\n",
    "\n",
    "count_dict = {}\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "for item in tmp_dict:\n",
    "    if tmp_dict[item] not in count_dict:\n",
    "        count_dict[tmp_dict[item]] = 1\n",
    "    else:\n",
    "        count_dict[tmp_dict[item]] = count_dict[tmp_dict[item]] + 1\n",
    "        \n",
    "tmp = pd.DataFrame.from_dict(count_dict,orient='index',columns=[\"Count\"])\n",
    "tmp[\"Percentage Match\"] = tmp.apply(lambda row: 100*row.Count/1000,axis=1)\n",
    "tmp.index.names = ['analysis/Consensus/'+args.input_file[19:-18]]\n",
    "tmp = tmp.sort_values(by=\"Count\",ascending=False)\n",
    "\n",
    "if args.verbose:\n",
    "    print(tmp)\n",
    "\n",
    "tmp.to_csv(args.input_file[:-18]+'match_distribution.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../record_name.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the record.name of entries in a fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "    \n",
    "with open(args.input_file) as original, open(args.input_file[:-15]+'labelled_read_pool.fasta', 'w') as corrected:\n",
    "    records = SeqIO.parse(original, 'fasta')\n",
    "    for record in records:\n",
    "        record.description = (args.input_file.split('/')[-3]+'_'+args.input_file.split('/')[-2])\n",
    "        SeqIO.write(record, corrected, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"../../analysis/Alignment/20171207_FAH18654/barcode12/1000_reads_qiime.paf\",\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()  \n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp\n",
    "tmp_dict = {}\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[line.split(\",\")[0]] = str(line.split(\",\")[5].split('_')[0])\n",
    "tmp2 = pd.DataFrame.from_dict(data=tmp_dict, orient='index')\n",
    "counts = tmp2[0].value_counts()\n",
    "tmp3 = counts.to_frame().reset_index(drop=False)\n",
    "g=open(\"../../database/sh_taxonomy_qiime_ver8_dynamic_02.02.2019.txt\", \"r\")\n",
    "if g.mode == \"r\":\n",
    "    contents2=g.read()\n",
    "tmp4=contents2.replace(\"\\t\",\",\").split(\"\\n\")\n",
    "tmp5 = []\n",
    "for i in range(0,len(tmp4)):\n",
    "    tmp5.append(tmp4[i].split(','))\n",
    "\n",
    "tmp6 = pd.DataFrame(tmp5)\n",
    "print(tmp3)\n",
    "\n",
    "tmp_dict2 = {}\n",
    "for i in range(0,len(tmp6[0])):\n",
    "    tmp_dict2[tmp6[0][i].split(\"_\")[0]]=tmp6[1][i]\n",
    "\n",
    "for a,b in tmp3[:7].itertuples(index=False):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    if a in tmp_dict2:\n",
    "        print(tmp_dict2[a]+'\\n')\n",
    "    else:\n",
    "        print(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../phylogenetic_naming_qscore.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the title of entries in the consensus fasta\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "true_names = {'20171103_FAH15473/barcode01': 'Puccinia_striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'Clavispora_lusitaniae',\n",
    "             '20171103_FAH15473/barcode09': 'Cryptococcus_neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Blastobotrys_proliferans',\n",
    "             '20171207_FAH18654/barcode02': 'Candida_zeylanoides',\n",
    "             '20171207_FAH18654/barcode03': 'Galactomyces_geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Meyerozyma_guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Zygoascus_hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Aspergillus_flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'Aspergillus_sp.',\n",
    "             '20171212_FAH18688/barcode03': 'CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Diaporthe_sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Tapesia_yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Tapesia_yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Entoleuca_sp.',\n",
    "             '20171212_FAH18688/barcode11': 'CCL060',\n",
    "             '20171212_FAH18688/barcode12': 'CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Cladophialophora_sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Candida_parapsilosis',\n",
    "             '20180108_FAH18647/barcode07': 'Cryptococcus_gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Geotrichum_candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Kluyveromyces_lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Pichia_membranifaciens'}\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "with open(args.input_file) as original, open(args.input_file[:-21]+'qscore_database_labelled.fasta', 'w+') as labelled:\n",
    "    for line in original:\n",
    "        if line.startswith(\">\"):\n",
    "            barcode = line.split('/')[2]+'/'+line.split('/')[3]\n",
    "            species = true_names[barcode]\n",
    "            newline = '>'+species+'\\n'\n",
    "        else:\n",
    "            newline = line\n",
    "        labelled.write(newline)\n",
    "original.close()\n",
    "labelled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../phylogenetic_naming_100.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the title of entries in the consensus fasta\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "true_names = {'20171103_FAH15473/barcode01': 'Puccinia_striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'Clavispora_lusitaniae',\n",
    "             '20171103_FAH15473/barcode09': 'Cryptococcus_neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Blastobotrys_proliferans',\n",
    "             '20171207_FAH18654/barcode02': 'Candida_zeylanoides',\n",
    "             '20171207_FAH18654/barcode03': 'Galactomyces_geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Meyerozyma_guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Zygoascus_hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Aspergillus_flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'Aspergillus_sp.',\n",
    "             '20171212_FAH18688/barcode03': 'CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Diaporthe_sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Tapesia_yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Tapesia_yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Entoleuca_sp.',\n",
    "             '20171212_FAH18688/barcode11': 'CCL060',\n",
    "             '20171212_FAH18688/barcode12': 'CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Cladophialophora_sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Candida_parapsilosis',\n",
    "             '20180108_FAH18647/barcode07': 'Cryptococcus_gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Geotrichum_candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Kluyveromyces_lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Pichia_membranifaciens'}\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "with open(args.input_file) as original, open(args.input_file[:-25]+'custom_database_100_labelled.fasta', 'w+') as labelled:\n",
    "    for line in original:\n",
    "        if line.startswith(\">\"):\n",
    "            barcode = line.split('/')[2]+'/'+line.split('/')[3]\n",
    "            species = true_names[barcode]\n",
    "            newline = '>'+species+'\\n'\n",
    "        else:\n",
    "            newline = line\n",
    "        labelled.write(newline)\n",
    "original.close()\n",
    "labelled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../phylogenetic_naming_101.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the title of entries in the consensus fasta\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "true_names = {'20171103_FAH15473/barcode01': 'Puccinia_striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'Clavispora_lusitaniae',\n",
    "             '20171103_FAH15473/barcode09': 'Cryptococcus_neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Blastobotrys_proliferans',\n",
    "             '20171207_FAH18654/barcode02': 'Candida_zeylanoides',\n",
    "             '20171207_FAH18654/barcode03': 'Galactomyces_geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Meyerozyma_guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Zygoascus_hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Aspergillus_flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'Aspergillus_sp.',\n",
    "             '20171212_FAH18688/barcode03': 'CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Diaporthe_sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Tapesia_yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Tapesia_yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Entoleuca_sp.',\n",
    "             '20171212_FAH18688/barcode11': 'CCL060',\n",
    "             '20171212_FAH18688/barcode12': 'CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Cladophialophora_sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Candida_parapsilosis',\n",
    "             '20180108_FAH18647/barcode07': 'Cryptococcus_gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Geotrichum_candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Kluyveromyces_lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Pichia_membranifaciens'}\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "with open(args.input_file) as original, open(args.input_file[:-25]+'custom_database_101_labelled.fasta', 'w+') as labelled:\n",
    "    for line in original:\n",
    "        if line.startswith(\">\"):\n",
    "            barcode = line.split('/')[2]+'/'+line.split('/')[3]\n",
    "            species = true_names[barcode]\n",
    "            newline = '>'+species+'\\n'\n",
    "        else:\n",
    "            newline = line\n",
    "        labelled.write(newline)\n",
    "original.close()\n",
    "labelled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../phylogenetic_naming_1000.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the title of entries in the consensus fasta\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "true_names = {'20171103_FAH15473/barcode01': 'Puccinia_striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'Clavispora_lusitaniae',\n",
    "             '20171103_FAH15473/barcode09': 'Cryptococcus_neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Blastobotrys_proliferans',\n",
    "             '20171207_FAH18654/barcode02': 'Candida_zeylanoides',\n",
    "             '20171207_FAH18654/barcode03': 'Galactomyces_geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Meyerozyma_guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Zygoascus_hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Aspergillus_flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'Aspergillus_sp.',\n",
    "             '20171212_FAH18688/barcode03': 'CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Diaporthe_sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Tapesia_yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Tapesia_yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Entoleuca_sp.',\n",
    "             '20171212_FAH18688/barcode11': 'CCL060',\n",
    "             '20171212_FAH18688/barcode12': 'CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Cladophialophora_sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Candida_parapsilosis',\n",
    "             '20180108_FAH18647/barcode07': 'Cryptococcus_gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Geotrichum_candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Kluyveromyces_lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Pichia_membranifaciens'}\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "with open(args.input_file) as original, open(args.input_file[:-26]+'custom_database_1000_labelled.fasta', 'w+') as labelled:\n",
    "    for line in original:\n",
    "        if line.startswith(\">\"):\n",
    "            barcode = line.split('/')[2]+'/'+line.split('/')[3]\n",
    "            species = true_names[barcode]\n",
    "            newline = '>'+species+'\\n'\n",
    "        else:\n",
    "            newline = line\n",
    "        labelled.write(newline)\n",
    "original.close()\n",
    "labelled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../phylogenetic_naming.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the title of entries in the consensus fasta\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "true_names = {'20171103_FAH15473/barcode01': 'Puccinia_striiformis-tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'Clavispora_lusitaniae',\n",
    "             '20171103_FAH15473/barcode09': 'Kluyveromyces_unidentified',\n",
    "             '20171103_FAH15473/barcode10': 'Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Blastobotrys_proliferans',\n",
    "             '20171207_FAH18654/barcode02': 'Debaryomyces_unidentified',\n",
    "             '20171207_FAH18654/barcode03': 'Galactomyces_geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Meyerozyma_guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Zygoascus_hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Aspergillus_flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'Aspergillus_unidentified',\n",
    "             '20171212_FAH18688/barcode03': 'Diaporthe_CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Diaporthe_unidentified',\n",
    "             '20171212_FAH18688/barcode05': 'Oculimacula_yallundae-CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Oculimacula_yallundae-CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Entoleuca_unidentified',\n",
    "             '20171212_FAH18688/barcode11': 'Asteroma_CCL060',\n",
    "             '20171212_FAH18688/barcode12': 'Asteroma_CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Cladophialophora_unidentified',\n",
    "             '20180108_FAH18647/barcode03': 'Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Candida_parapsilosis',\n",
    "             '20180108_FAH18647/barcode07': 'Cryptococcus_gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Geotrichum_candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Candida_unidentified',\n",
    "             '20180108_FAH18647/barcode10': 'Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Pichia_membranifaciens'}\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "with open(args.input_file) as original, open(args.input_file[:-21]+'custom_database_labelled.fasta', 'w+') as labelled:\n",
    "    for line in original:\n",
    "        if line.startswith(\">\"):\n",
    "            barcode = line.split('/')[2]+'/'+line.split('/')[3]\n",
    "            species = true_names[barcode]\n",
    "            newline = '>'+species+'\\n'\n",
    "        else:\n",
    "            newline = line\n",
    "        labelled.write(newline)\n",
    "original.close()\n",
    "labelled.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../phylogenetic_naming_final.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This program alters the title of entries in the consensus fasta\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "true_names = {'20171103_FAH15473/barcode01': 'Puccinia_striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'Clavispora_lusitaniae',\n",
    "             '20171103_FAH15473/barcode09': 'Cryptococcus_neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Blastobotrys_proliferans',\n",
    "             '20171207_FAH18654/barcode02': 'Candida_zeylanoides',\n",
    "             '20171207_FAH18654/barcode03': 'Galactomyces_geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Meyerozyma_guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Zygoascus_hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Aspergillus_flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'Aspergillus_sp.',\n",
    "             '20171212_FAH18688/barcode03': 'CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Diaporthe_sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Tapesia_yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Tapesia_yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Entoleuca_sp.',\n",
    "             '20171212_FAH18688/barcode11': 'CCL060',\n",
    "             '20171212_FAH18688/barcode12': 'CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Cladophialophora_sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Candida_parapsilosis',\n",
    "             '20180108_FAH18647/barcode07': 'Cryptococcus_gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Geotrichum_candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Kluyveromyces_lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Pichia_membranifaciens'}\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "with open(args.input_file) as original, open(args.input_file[:-25]+'final_custom_database_labelled.fasta', 'w+') as labelled:\n",
    "    for line in original:\n",
    "        if line.startswith(\">\"):\n",
    "            barcode = line.split('/')[2]+'/'+line.split('/')[3]\n",
    "            species = true_names[barcode]\n",
    "            newline = '>'+species+'\\n'\n",
    "        else:\n",
    "            newline = line\n",
    "        labelled.write(newline)\n",
    "original.close()\n",
    "labelled.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to generate consensus and tree\n",
    "\n",
    "    1. Subsample a set number of reads using the get_cons_ids python script line\n",
    "    2. From these reads, generate an alignment file using muscle via the create_consensus script line, on the Consensus/ folder\n",
    "    3. Use the emboss_cons script line to create a consensus from the alignment file, from the analysis folder\n",
    "        3.5 This also involves a cleanup phase where any 'n' or 'N' character is removed from the resulting file\n",
    "    4. Collect all cleaned consensus files in a single fasta file using the make_database.sh script\n",
    "    5. Rename each file using the phylogenetic_naming script line\n",
    "    6. Use muscle to create an alignment file (muscle -in labelled_consensus.fasta -out labelled_consensus.aln)\n",
    "    7. Download file and export to Geneious\n",
    "    8. Utilise the Mask Alignment tool to remove gaps\n",
    "    9. Create tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylogeny = {'20171103_FAH15473/barcode01': 'Basidiomycota/Pucciniomycetes/Pucciniales/Pucciniaceae/Puccinia/striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Ascomycota//Dothideomycetes/Capnodiales/Mycosphaerellaceae/Zymoseptoria/tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Ascomycota/Dothideomycetes/Pleosporales/Pleospraceae/Pyrenophora/tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Ascomycota/Sordariomycetes/Hypocreales/Nectriaceae/Fusarium/oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Ascomycota/Pezizomycetes/Pezizales/Tuberaceae/Tuber/brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Basidiomycota/Agaricales/Cortinariaceae/Cortinarius/globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/niger',\n",
    "             '20171103_FAH15473/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Clavispora/lusitaniae',\n",
    "                 '20171103_FAH15473/barcode09': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/sp. (Cryptococcus_neoformans)',\n",
    "             '20171103_FAH15473/barcode10': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Penicillium/chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Basidiomycota/Microbotryomycetes/Sporidiobolales/Sporidiobolaceae/Rhodotorula/mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Ascomycota/Sordariomycetes/Microascales/Microascaceae/Scedosporium/boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Blastobotrys/proliferans',\n",
    "                 '20171207_FAH18654/barcode02': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Meyerozyma/sp. (Candida_zeylanoides)',\n",
    "                 '20171207_FAH18654/barcode03': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Galactomyces/geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Kodamaea/ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Meyerozyma/guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Ascomycota/Saccharomycetes/Saccharomycetales/Phaffomycetaceae/Wickerhamomyces/anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Yarrowia/lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Zygoascus/hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/zero',\n",
    "             '20171212_FAH18688/barcode02': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/sp.',\n",
    "                 '20171212_FAH18688/barcode03': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Tapesia/yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Ascomycota/Dothideomycetes/Botryosphaeriales/Botryosphaeriaceae/Dothiorella/vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Exobasidiomycetes/Microstromatales/Quambalariaceae/Quambalaria/cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Ascomycota/Sordariomycetes/Xylariales.Xylariaceae/Entoleuca/sp.',\n",
    "                 '20171212_FAH18688/barcode11': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL060',\n",
    "                 '20171212_FAH18688/barcode12': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Saccharomyces/cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Ascomycota/Eurotiomycetes/Chaetothyriales/Herpotrichiellaceae/Cladophialophora/sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/parapsilosis',\n",
    "                 '20180108_FAH18647/barcode07': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/sp. (Cryptococcus_gattii)',\n",
    "             '20180108_FAH18647/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Geotrichum/candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/membranifaciens'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../test_reads_vs_database.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Enter the analysis directory to generate a dataframe that extracts the details\n",
    "for each sample\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "phylogeny = {'20171103_FAH15473/barcode01': 'Basidiomycota/Pucciniomycetes/Pucciniales/Pucciniaceae/Puccinia/striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Ascomycota/Dothideomycetes/Capnodiales/Mycosphaerellaceae/Zymoseptoria/tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Ascomycota/Dothideomycetes/Pleosporales/Pleospraceae/Pyrenophora/tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Ascomycota/Sordariomycetes/Hypocreales/Nectriaceae/Fusarium/oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Ascomycota/Pezizomycetes/Pezizales/Tuberaceae/Tuber/brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Basidiomycota/Agaricomycetes/Agaricales/Cortinariaceae/Cortinarius/globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/niger',\n",
    "             '20171103_FAH15473/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Clavispora/lusitaniae',\n",
    "                 '20171103_FAH15473/barcode09': 'Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Penicillium/chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Basidiomycota/Microbotryomycetes/Sporidiobolales/Sporidiobolaceae/Rhodotorula/mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Ascomycota/Sordariomycetes/Microascales/Microascaceae/Scedosporium/boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Blastobotrys/proliferans',\n",
    "                 '20171207_FAH18654/barcode02': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/zeylanoides',\n",
    "                 '20171207_FAH18654/barcode03': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Galactomyces/geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Kodamaea/ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Meyerozyma/guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Ascomycota/Saccharomycetes/Saccharomycetales/Phaffomycetaceae/Wickerhamomyces/anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Yarrowia/lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Zygoascus/hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/zero',\n",
    "             '20171212_FAH18688/barcode02': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/sp.',\n",
    "                 '20171212_FAH18688/barcode03': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Oculimacula/yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Oculimacula/yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Ascomycota/Dothideomycetes/Botryosphaeriales/Botryosphaeriaceae/Dothiorella/vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Exobasidiomycetes/Microstromatales/Quambalariaceae/Quambalaria/cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Ascomycota/Sordariomycetes/Xylariales.Xylariaceae/Entoleuca/sp.',\n",
    "                 '20171212_FAH18688/barcode11': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL060',\n",
    "                 '20171212_FAH18688/barcode12': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Saccharomyces/cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Ascomycota/Eurotiomycetes/Chaetothyriales/Herpotrichiellaceae/Cladophialophora/sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/parapsilosis',\n",
    "                 '20180108_FAH18647/barcode07': 'Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Geotrichum/candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/membranifaciens'}\n",
    "input_file = args.input_file\n",
    "\n",
    "print(input_file+'\\n')\n",
    "\n",
    "f=open(input_file,\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()  \n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp\n",
    "tmp_dict = {}\n",
    "counter=0\n",
    "\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[counter] = str(line.split(\",\")[5].split('_')[0])\n",
    "    counter+=1\n",
    "tmp2 = pd.DataFrame.from_dict(data=tmp_dict, orient='index')\n",
    "counts = tmp2[0].value_counts()\n",
    "tmp3 = counts.to_frame().reset_index(drop=False)\n",
    "\n",
    "g=open(\"database/sh_taxonomy_qiime_ver8_dynamic_02.02.2019.txt\", \"r\")\n",
    "if g.mode == \"r\":\n",
    "    contents2=g.read()\n",
    "tmp4=contents2.replace(\"\\t\",\",\").split(\"\\n\")\n",
    "\n",
    "tmp5 = []\n",
    "for i in range(0,len(tmp4)):\n",
    "    tmp5.append(tmp4[i].split(','))\n",
    "\n",
    "tmp6 = pd.DataFrame(tmp5)\n",
    "\n",
    "tmp_dict2 = {}\n",
    "for i in range(0,len(tmp6[0])):\n",
    "    tmp_dict2[tmp6[0][i].split(\"_\")[0]]=tmp6[1][i]\n",
    "\n",
    "    \n",
    "tobewritten = pd.DataFrame(data=[],columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'])\n",
    "\n",
    "for a,b in tmp3[:7].itertuples(index=False):\n",
    "    # a == id, b == count\n",
    "    if a in tmp_dict2:\n",
    "        tmp_specs = []\n",
    "        tmp_specs.append(b)\n",
    "        if len(tmp_dict2[a].split('s__')[-1].split('_')) > 1:\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[1])\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[0])\n",
    "        else:\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[0])\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-1].split(';')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-2].split('o__')[-1].split(';')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-2].split('o__')[-2].split('c__')[-1].split(';')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-2].split('o__')[-2].split('c__')[-2].split('p__')[-1].split(';')[0])\n",
    "        tmpf = pd.DataFrame([tmp_specs],columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'],index=[a])\n",
    "        tobewritten = tobewritten.append(tmpf)\n",
    "\n",
    "correct_list = ['N/A', phylogeny[input_file[19:-19]].split('/')[-1].split('_')[0], phylogeny[input_file[19:-19]].split('/')[-2],phylogeny[input_file[19:-19]].split('/')[-3],phylogeny[input_file[19:-19]].split('/')[-4],phylogeny[input_file[19:-19]].split('/')[-5],phylogeny[input_file[19:-19]].split('/')[-6]]\n",
    "correct = pd.DataFrame([correct_list], columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'],index=['Suggested'])\n",
    "tobewritten = tobewritten.append(correct)\n",
    "\n",
    "consensus = [0]*7\n",
    "count = 0\n",
    "for line in tobewritten:\n",
    "    if line != 'Count':\n",
    "        tmps = tobewritten[line].value_counts().to_frame()\n",
    "        consensus[count] = tmps.index[0]\n",
    "        count += 1\n",
    "    else:\n",
    "        consensus[count] = 'N/A'\n",
    "        count += 1\n",
    "consensus = pd.DataFrame([consensus], columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'],index=['Consensus Match'])\n",
    "tobewritten = tobewritten.append([consensus])\n",
    "\n",
    "if (correct.values[0][1:2] == consensus.values[0][1:2]).all():\n",
    "    print('\\033[0;31m'+\"SPECIES MATCH\"+'\\033[1;37m')\n",
    "else:\n",
    "    print('\\033[0;34m'+\"NO SPECIES MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "if (correct.values[0][2:3] == consensus.values[0][2:3]).all():\n",
    "    print('\\033[0;31m'+\"GENUS MATCH\"+'\\033[1;37m')\n",
    "else:\n",
    "    print('\\033[0;34m'+\"NO GENUS MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "if (correct.values[0][3:4] == consensus.values[0][3:4]).all():\n",
    "    print('\\033[0;31m'+\"FAMILY MATCH\"+'\\033[1;37m')\n",
    "else:\n",
    "    print('\\033[0;34m'+\"NO FAMILY MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "if (correct.values[0][4:5] == consensus.values[0][4:5]).all():\n",
    "    print('\\033[0;31m'+\"ORDER MATCH\"+'\\033[1;37m')\n",
    "else:\n",
    "    print('\\033[0;34m'+\"NO ORDER MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "if (correct.values[0][5:6] == consensus.values[0][5:6]).all():\n",
    "    print('\\033[0;31m'+\"CLASS MATCH\"+'\\033[1;37m')\n",
    "else:\n",
    "    print('\\033[0;34m'+\"NO CLASS MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "if (correct.values[0][6:] == consensus.values[0][6:]).all():\n",
    "    print('\\033[0;31m'+\"PHYLUM MATCH\"+'\\033[1;37m')\n",
    "else:\n",
    "    print('\\033[0;34m'+\"NO PHYLUM MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "tobewritten.to_csv(input_file[:-18]+'reads_matches_found.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../test_consensus_vs_database.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Enter the analysis directory to generate a dataframe that extracts the details\n",
    "for each sample\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "phylogeny = {'20171103_FAH15473/barcode01': 'Basidiomycota/Pucciniomycetes/Pucciniales/Pucciniaceae/Puccinia/striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Ascomycota/Dothideomycetes/Capnodiales/Mycosphaerellaceae/Zymoseptoria/tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Ascomycota/Dothideomycetes/Pleosporales/Pleospraceae/Pyrenophora/tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Ascomycota/Sordariomycetes/Hypocreales/Nectriaceae/Fusarium/oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Ascomycota/Pezizomycetes/Pezizales/Tuberaceae/Tuber/brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Basidiomycota/Agaricomycetes/Agaricales/Cortinariaceae/Cortinarius/globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/niger',\n",
    "             '20171103_FAH15473/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Clavispora/lusitaniae',\n",
    "                 '20171103_FAH15473/barcode09': 'Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Penicillium/chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Basidiomycota/Microbotryomycetes/Sporidiobolales/Sporidiobolaceae/Rhodotorula/mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Ascomycota/Sordariomycetes/Microascales/Microascaceae/Scedosporium/boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Blastobotrys/proliferans',\n",
    "                 '20171207_FAH18654/barcode02': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/zeylanoides',\n",
    "                 '20171207_FAH18654/barcode03': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Galactomyces/geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Ascomycota/Saccharomycetes/Saccharomycetales/Metschnikowiaceae/Kodamaea/ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Meyerozyma/guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Ascomycota/Saccharomycetes/Saccharomycetales/Phaffomycetaceae/Wickerhamomyces/anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Debaryomycetaceae/Yamadazyma/scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Yarrowia/lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Ascomycota/Saccharomycetes/Saccharomycetales/Trichomonascaceae/Zygoascus/hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/zero',\n",
    "             '20171212_FAH18688/barcode02': 'Ascomycota/Eurotiomycetes/Eurotiales/Trichocomaceae/Aspergillus/sp.',\n",
    "                 '20171212_FAH18688/barcode03': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Oculimacula/yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Ascomycota/Leotiomycetes/Helotiales/Dermateaceae/Oculimacula/yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Ascomycota/Dothideomycetes/Botryosphaeriales/Botryosphaeriaceae/Dothiorella/vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Exobasidiomycetes/Microstromatales/Quambalariaceae/Quambalaria/cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Ascomycota/Sordariomycetes/Xylariales.Xylariaceae/Entoleuca/sp.',\n",
    "                 '20171212_FAH18688/barcode11': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL060',\n",
    "                 '20171212_FAH18688/barcode12': 'Ascomycota/Sordariomycetes/Diaporthales/Diaporthaceae/Diaporthe/sp._CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Saccharomyces/cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Ascomycota/Eurotiomycetes/Chaetothyriales/Herpotrichiellaceae/Cladophialophora/sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Candida/parapsilosis',\n",
    "                 '20180108_FAH18647/barcode07': 'Basidiomycota/Tremellomycetes/Tremellales/Tremellaceae/Cryptococcus/gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Ascomycota/Saccharomycetes/Saccharomycetales/Dipodascaceae/Geotrichum/candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Ascomycota/Saccharomycetes/Saccharomycetales/Saccharomycetaceae/Kluyveromyces/marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Ascomycota/Saccharomycetes/Saccharomycetales/Pichiaceae/Pichia/membranifaciens'}\n",
    "input_file = args.input_file\n",
    "\n",
    "print(input_file+'\\n')\n",
    "\n",
    "f=open(input_file,\"r\")\n",
    "if f.mode == \"r\":\n",
    "    contents=f.read()  \n",
    "tmp=contents.replace(\"\\t\",\",\").split('\\n')\n",
    "tmp\n",
    "tmp_dict = {}\n",
    "counter=0\n",
    "\n",
    "for line in tmp[:-1]:\n",
    "    tmp_dict[counter] = str(line.split(\",\")[5].split('_')[0])\n",
    "    counter+=1\n",
    "tmp2 = pd.DataFrame.from_dict(data=tmp_dict, orient='index')\n",
    "counts = tmp2[0].value_counts()\n",
    "tmp3 = counts.to_frame().reset_index(drop=False)\n",
    "\n",
    "g=open(\"database/sh_taxonomy_qiime_ver8_dynamic_02.02.2019.txt\", \"r\")\n",
    "if g.mode == \"r\":\n",
    "    contents2=g.read()\n",
    "tmp4=contents2.replace(\"\\t\",\",\").split(\"\\n\")\n",
    "\n",
    "tmp5 = []\n",
    "for i in range(0,len(tmp4)):\n",
    "    tmp5.append(tmp4[i].split(','))\n",
    "\n",
    "tmp6 = pd.DataFrame(tmp5)\n",
    "\n",
    "tmp_dict2 = {}\n",
    "for i in range(0,len(tmp6[0])):\n",
    "    tmp_dict2[tmp6[0][i].split(\"_\")[0]]=tmp6[1][i]\n",
    "\n",
    "    \n",
    "tobewritten = pd.DataFrame(data=[],columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'])\n",
    "\n",
    "for a,b in tmp3[:7].itertuples(index=False):\n",
    "    # a == id, b == count\n",
    "    if a in tmp_dict2:\n",
    "        tmp_specs = []\n",
    "        tmp_specs.append(b)\n",
    "        if len(tmp_dict2[a].split('s__')[-1].split('_')) > 1:\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[1])\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[0])\n",
    "        else:\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[0])\n",
    "            tmp_specs.append(tmp_dict2[a].split('s__')[-1].split('_')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-1].split(';')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-2].split('o__')[-1].split(';')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-2].split('o__')[-2].split('c__')[-1].split(';')[0])\n",
    "        tmp_specs.append(tmp_dict2[a].split('s__')[-2].split('g__')[-2].split('f__')[-2].split('o__')[-2].split('c__')[-2].split('p__')[-1].split(';')[0])\n",
    "        tmpf = pd.DataFrame([tmp_specs],columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'],index=[a])\n",
    "        tobewritten = tobewritten.append(tmpf)\n",
    "\n",
    "        \n",
    "pd.set_option('display.max_rows', None)\n",
    "print('\\033[0;34m')\n",
    "print(tobewritten)\n",
    "print('\\033[1;37m')       \n",
    "# correct_list = ['N/A', phylogeny[input_file[19:-33]].split('/')[-1].split('_')[0], phylogeny[input_file[19:-33]].split('/')[-2],phylogeny[input_file[19:-33]].split('/')[-3],phylogeny[input_file[19:-33]].split('/')[-4],phylogeny[input_file[19:-33]].split('/')[-5],phylogeny[input_file[19:-33]].split('/')[-6]]\n",
    "# correct = pd.DataFrame([correct_list], columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'],index=['Suggested'])\n",
    "\n",
    "# tps1 = correct[['Species Match']]\n",
    "# tps2 = tobewritten[['Species Match']]\n",
    "# tpg1 = correct[['Genus Match']]\n",
    "# tpg2 = tobewritten[['Genus Match']]\n",
    "# print(pd.merge(tps1,tps2,on='Species Match'))\n",
    "# if pd.merge(tps1,tps2,on='Species Match').any().any():\n",
    "#     print('\\033[0;34m')\n",
    "#     print(pd.merge(tps1,tps2,on='Species Match').any())\n",
    "# else:\n",
    "#     print('\\033[0;31m')\n",
    "#     print(pd.merge(tps1,tps2,on='Species Match').any())\n",
    "# print('\\033[1;37m')\n",
    "\n",
    "# print(pd.merge(tpg1,tpg2,on='Genus Match'))\n",
    "# if pd.merge(tpg1,tpg2,on='Genus Match').any().any():\n",
    "#     print('\\033[0;34m')\n",
    "#     print(pd.merge(tpg1,tpg2,on='Genus Match').any())\n",
    "# else:\n",
    "#     print('\\033[0;31m')\n",
    "#     print(pd.merge(tpg1,tpg2,on='Genus Match').any())\n",
    "# print('\\033[1;37m')\n",
    "\n",
    "\n",
    "\n",
    "# consensus = [0]*7\n",
    "# count = 0\n",
    "# total = 0\n",
    "# match = 0\n",
    "# for line in tobewritten:\n",
    "#     if line != 'Count':\n",
    "#         tmps = tobewritten[line].value_counts().to_frame()\n",
    "#         if tmps.index[0] in correct_list:\n",
    "#             consensus[count] = tmps.index[0]\n",
    "#             print(consensus[count])\n",
    "#         else:\n",
    "#             print(\"NO\")\n",
    "#         count += 1\n",
    "#     else:\n",
    "#         consensus[count] = 'N/A'\n",
    "#         count += 1\n",
    "\n",
    "        \n",
    "# correct = pd.DataFrame([correct_list], columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'],index=['Suggested'])\n",
    "# tobewritten = tobewritten.append(correct)\n",
    "\n",
    "# consensus = pd.DataFrame([consensus], columns=['Count','Species Match','Genus Match','Family Match','Order Match','Class Match','Phylum Match'],index=['Consensus Match'])\n",
    "# tobewritten = tobewritten.append([consensus])\n",
    "\n",
    "# if (correct.values[0][1:2] == consensus.values[0][1:2]).all():\n",
    "#     print('\\033[0;31m'+\"SPECIES MATCH\"+'\\033[1;37m')\n",
    "# else:\n",
    "#     print('\\033[0;34m'+\"NO SPECIES MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "# if (correct.values[0][2:3] == consensus.values[0][2:3]).all():\n",
    "#     print('\\033[0;31m'+\"GENUS MATCH\"+'\\033[1;37m')\n",
    "# else:\n",
    "#     print('\\033[0;34m'+\"NO GENUS MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "# if (correct.values[0][3:4] == consensus.values[0][3:4]).all():\n",
    "#     print('\\033[0;31m'+\"FAMILY MATCH\"+'\\033[1;37m')\n",
    "# else:\n",
    "#     print('\\033[0;34m'+\"NO FAMILY MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "# if (correct.values[0][4:5] == consensus.values[0][4:5]).all():\n",
    "#     print('\\033[0;31m'+\"ORDER MATCH\"+'\\033[1;37m')\n",
    "# else:\n",
    "#     print('\\033[0;34m'+\"NO ORDER MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "# if (correct.values[0][5:6] == consensus.values[0][5:6]).all():\n",
    "#     print('\\033[0;31m'+\"CLASS MATCH\"+'\\033[1;37m')\n",
    "# else:\n",
    "#     print('\\033[0;34m'+\"NO CLASS MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "# if (correct.values[0][6:] == consensus.values[0][6:]).all():\n",
    "#     print('\\033[0;31m'+\"PHYLUM MATCH\"+'\\033[1;37m')\n",
    "# else:\n",
    "#     print('\\033[0;34m'+\"NO PHYLUM MATCH\"+'\\033[1;37m')\n",
    "    \n",
    "# print('\\n\\n')\n",
    "\n",
    "\n",
    "# tobewritten.to_csv(input_file[:-32]+'matches_found.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../overall_statistics.py\n",
    "\n",
    "\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\"\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "The goal of this program is to examine the distribution of reads within\n",
    "each file, and for the files generated after homology analysis.\n",
    "The program will generate summary statistics for the result of the homology\n",
    "analysis and save figures illustrating the read distribution for the\n",
    "frDNA reads\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"full_file\")\n",
    "args = parser.parse_args()\n",
    "print('\\033[0;35m'+'START'+'\\033[1;37m')\n",
    "full_file = args.full_file\n",
    "print(full_file)\n",
    "homology_folder = args.full_file[:9]+'Python_Processing'+args.full_file[21:-12]+'combined_test.paf'\n",
    "print(homology_folder)\n",
    "length_folder = args.full_file[:9]+'Length_Filtered'+args.full_file[21:-12]+'length_restricted_reads.fasta'\n",
    "print(length_folder)\n",
    "\n",
    "# Load the full file containing all reads for this barcode\n",
    "full_file_dict = SeqIO.to_dict(SeqIO.parse(full_file, \"fastq\"))\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + full_file + '\\033[1;37m')\n",
    "\n",
    "    \n",
    "# Open the original merged.fastq file\n",
    "# Extract the number of reads contained within this file\n",
    "full_lengths = []\n",
    "for key in full_file_dict:\n",
    "    full_lengths.append(len(full_file_dict[key].seq))\n",
    "full_lengths_len = len(full_file_dict)\n",
    "print(full_lengths_len)\n",
    "\n",
    "\n",
    "# Import the PAF file resulting from the minimap2 homology filtering\n",
    "homology_paf = pd.read_csv(homology_folder, sep='\\t', header=None, engine='python')\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + homology_folder + '\\033[1;37m')\n",
    "          \n",
    "# Create new dictionary using only keys present in paf file\n",
    "# Extract the number of reads contained within this file\n",
    "homology_dict = {}\n",
    "for key in homology_paf[0].unique():\n",
    "    homology_dict[key] = full_file_dict[key]\n",
    "\n",
    "homology_lengths = []\n",
    "for key in homology_dict:\n",
    "          homology_lengths.append(len(homology_dict[key].seq))\n",
    "homology_lengths_len = len(homology_lengths)\n",
    "print(homology_lengths_len)\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Loaded \" + length_folder + '\\033[1;37m')\n",
    "    \n",
    "# Open length restricted fasta\n",
    "# Extract the number of reads contained within this file\n",
    "length_dict = SeqIO.to_dict(SeqIO.parse(length_folder, \"fasta\"))\n",
    "\n",
    "length_lengths = []\n",
    "for key in length_dict:\n",
    "    length_lengths.append(len(length_dict[key].seq))\n",
    "length_lengths_len = len(length_dict)\n",
    "print(length_lengths_len)\n",
    "\n",
    "number_of_reads = pd.DataFrame([[full_lengths_len, homology_lengths_len, length_lengths_len]], columns=['Original # reads','# reads after homology filtering','# reads after length filtering'])\n",
    "number_of_reads.to_csv(args.full_file[:9]+'Stats'+args.full_file[21:-12]+'read_numbers.csv', index=False)\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;32m' + \"Number of reads file saved to \" + args.full_file[:9]+'Stats'+args.full_file[21:-12]+'read_numbers.csv' + '\\033[1;37m')\n",
    "\n",
    "fields = [full_lengths_len, homology_lengths_len, length_lengths_len]\n",
    "if args.full_file[40:-13] != 'unclassified':\n",
    "    if '20171212_FAH18688/barcode10' not in args.full_file[22:-13] and '20171207_FAH18654/barcode10' not in args.full_file[22:-13]:\n",
    "        with open(args.full_file[:9]+'Stats/read_numbers.csv','a+') as fd:\n",
    "            writer = csv.writer(fd)\n",
    "            writer.writerow(fields)\n",
    "print('\\033[0;35m'+'END'+'\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_stats = pd.read_csv('../../analysis/Stats/read_number.csv', header=None, engine='python')\n",
    "print(number_stats)\n",
    "print(np.mean(number_stats[0]))\n",
    "print(np.mean(number_stats[1]))\n",
    "print(min(number_stats[2]))\n",
    "print(max(number_stats[2]))\n",
    "print(np.mean(number_stats[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(tmp['read_id'][1:])):\n",
    "    tmps = pd.DataFrame(data=[tmp['read_id'][i+1:],tmp['mean_qscore_template'][i+1:]])\n",
    "    id_scores.append(tmps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv(\"../../analysis/Basecalled/20171103_FAH15473/barcode01/sequencing_summary.txt\",sep='\\t', header=None, names=['filename', 'read_id', 'run_id', 'batch_id', 'channel', 'mux', 'start_time', 'duration', 'num_events', 'passes_filtering', 'template_start', 'num_events_template', 'template_duration', 'sequence_length_template', 'mean_qscore_template', 'strand_score_template', 'median_template', 'mad_template'], engine='python')\n",
    "tmpd = SeqIO.to_dict(SeqIO.parse(\"../../analysis/Length_Filtered/20171103_FAH15473/barcode01/length_restricted_reads.fasta\", \"fasta\"))\n",
    "keys = []\n",
    "for key in tmpd:\n",
    "    keys.append(key)\n",
    "id_score = pd.DataFrame(data=None,columns=['read_id','mean_qscore_template'])\n",
    "tmp_keys = {}\n",
    "for i in range(1,len(tmp)):\n",
    "    tmp_keys[tmp['read_id'][i]] = tmp['mean_qscore_template'][i]\n",
    "for key in keys:\n",
    "    if key in tmp_keys:\n",
    "        tmpf = pd.DataFrame(data=[[key,tmp_keys[key]]],columns=['read_id','mean_qscore_template'])\n",
    "    id_score = id_score.append(tmpf)\n",
    "id_score = id_score.sort_values(by=['mean_qscore_template'])\n",
    "new_ids_list = id_score['read_id'][:100]\n",
    "\n",
    "new_dict = {}\n",
    "for key in new_ids_list:\n",
    "    new_dict[key] = tmpd[key]\n",
    "SeqIO.write(new_dict.values(), \"../../analysis/Consensus/20171103_FAH15473/barcode01/high_qscore_100.fasta\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../for_consensus_high_qscore_100.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from mothur_py import Mothur\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "WHITE='\\033[1;37m'\n",
    "BLUE='\\033[0;34m'\n",
    "RED='\\033[0;31m'\n",
    "NC='\\033[0m'\n",
    "GREEN='\\033[0;32m'\n",
    "PURPLE='\\033[1;35m'\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Input the sequencing_summary.txt file from Basecalled\n",
    "Read in the length_restricted_reads.fasta from Length_Filtered\n",
    "Save to Consensus\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;31m' + \"Input file is \" + args.input_file + '\\033[1;37m')\n",
    "\n",
    "    \n",
    "tmp = pd.read_csv(args.input_file,sep='\\t', header=None, names=['filename', 'read_id', 'run_id', 'batch_id', 'channel', 'mux', 'start_time', 'duration', 'num_events', 'passes_filtering', 'template_start', 'num_events_template', 'template_duration', 'sequence_length_template', 'mean_qscore_template', 'strand_score_template', 'median_template', 'mad_template'], engine='python')\n",
    "tmpd = SeqIO.to_dict(SeqIO.parse(\"Length_Filtered\"+args.input_file[10:-22]+\"length_restricted_reads.fasta\", \"fasta\"))\n",
    "keys = []\n",
    "for key in tmpd:\n",
    "    keys.append(key)\n",
    "id_score = pd.DataFrame(data=None,columns=['read_id','mean_qscore_template'])\n",
    "tmp_keys = {}\n",
    "for i in range(1,len(tmp)):\n",
    "    tmp_keys[tmp['read_id'][i]] = tmp['mean_qscore_template'][i]\n",
    "for key in keys:\n",
    "    if key in tmp_keys:\n",
    "        tmpf = pd.DataFrame(data=[[key,tmp_keys[key]]],columns=['read_id','mean_qscore_template'])\n",
    "    id_score = id_score.append(tmpf)\n",
    "id_score = id_score.sort_values(by=['mean_qscore_template'])\n",
    "new_ids_list = id_score['read_id'][:100]\n",
    "\n",
    "new_dict = {}\n",
    "for key in new_ids_list:\n",
    "    new_dict[key] = tmpd[key]\n",
    "SeqIO.write(new_dict.values(), \"Consensus\"+args.input_file[10:-22]+\"high_qscore_100.fasta\", \"fasta\")\n",
    "    \n",
    "\n",
    "if args.verbose:\n",
    "    print('\\033[0;34m' + \"Reads saved to \" + '\\033[0;35m' + \"Consensus\"+args.input_file[10:-22]+\"high_qscore_100.fasta\" + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map genetic distance on x vs crossmapping percentage on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylogeny = {'20171103_FAH15473/barcode01': 'Puccinia_striiformis_tritici',\n",
    "             '20171103_FAH15473/barcode02': 'Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'Clavispora_lusitaniae',\n",
    "                 '20171103_FAH15473/barcode09': 'Cryptococcus_neoformans',\n",
    "             '20171103_FAH15473/barcode10': 'Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'Blastobotrys_proliferans',\n",
    "                 '20171207_FAH18654/barcode02': 'Candida_zeylanoides',\n",
    "                 '20171207_FAH18654/barcode03': 'Galactomyces_geotrichum',\n",
    "             '20171207_FAH18654/barcode04': 'Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'Meyerozyma_guillermondii',\n",
    "             '20171207_FAH18654/barcode06': 'Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'Zygoascus_hellenicus',\n",
    "             '20171207_FAH18654/barcode12': 'Aspergillus_flavus',\n",
    "             '20171212_FAH18688/barcode01': 'Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'Aspergillus_sp.',\n",
    "                 '20171212_FAH18688/barcode03': 'CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'Diaporthe_sp.',\n",
    "             '20171212_FAH18688/barcode05': 'Tapesia_yallundae_CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'Tapesia_yallundae_CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'Entoleuca_sp.',\n",
    "                 '20171212_FAH18688/barcode11': 'CCL060',\n",
    "                 '20171212_FAH18688/barcode12': 'CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'Cladophialophora_sp.',\n",
    "             '20180108_FAH18647/barcode03': 'Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'Candida_parapsilosis',\n",
    "                 '20180108_FAH18647/barcode07': 'Cryptococcus_gattii',\n",
    "             '20180108_FAH18647/barcode08': 'Geotrichum_candidum',\n",
    "             '20180108_FAH18647/barcode09': 'Kluyveromyces_lactis',\n",
    "             '20180108_FAH18647/barcode10': 'Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'Pichia_membranifaciens'}\n",
    "\n",
    "large_frame = pd.read_csv(\"../../analysis/Alignment/database.csv\", header=0, index_col=0, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/media/MassStorage/tmp/TE/honours/analysis/Alignment/*/*/match_distribution.csv\"\n",
    "path_names = glob.glob(path)\n",
    "for path in path_names:\n",
    "    template_frame = pd.DataFrame(data=None, columns = ['Species Name','Genetic Distance','Crossmapping Fraction'])\n",
    "    if path[71:-23] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            match_dist = pd.read_csv(path, header=0)\n",
    "            for element in match_dist.index:\n",
    "#                 print(match_dist.at[element,'analysis/Consensus/'+path[53:-22]][19:-1])\n",
    "#                 print(match_dist.at[element,'Percentage Match'])\n",
    "                tmp = pd.DataFrame(data=[[phylogeny[match_dist.at[element,'analysis/Consensus/'+path[53:-22]][19:-1]],None,match_dist.at[element,'Percentage Match']]],columns=['Species Name','Genetic Distance','Crossmapping Fraction'])\n",
    "                template_frame = template_frame.append(tmp,ignore_index=True)\n",
    "            template_frame = template_frame.set_index('Species Name')\n",
    "            for element in template_frame.index:\n",
    "                if np.isnan(large_frame.at[element,phylogeny[path[53:-23]]]):\n",
    "                    template_frame.at[element,'Genetic Distance'] = 0\n",
    "                else:\n",
    "                    template_frame.at[element,'Genetic Distance'] = 100-large_frame.at[element,phylogeny[path[53:-23]]]\n",
    "            figures = sns.scatterplot(x=template_frame['Genetic Distance'],y=template_frame['Crossmapping Fraction'])\n",
    "            figures.set_title(\"Crossmapping Fraction vs Genetic Distance across Species\", fontsize=15)\n",
    "            ax = figures.get_figure()\n",
    "            ax.savefig(\"../../analysis/Stats/crossmapvsgendist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = \"/media/MassStorage/tmp/TE/honours/analysis/Alignment/*/*/match_distribution.csv\"\n",
    "path_names = glob.glob(path)\n",
    "for path in path_names:\n",
    "    template_frame = pd.DataFrame(data=None, columns = ['Species Name','Genetic Distance','Crossmapping Fraction'])\n",
    "    if path[71:-23] != 'unclassified':\n",
    "        if '20171212_FAH18688/barcode10' not in path and '20171207_FAH18654/barcode10' not in path:\n",
    "            match_dist = pd.read_csv(path, header=0)\n",
    "            for element in match_dist.index:\n",
    "#                 print(match_dist.at[element,'analysis/Consensus/'+path[53:-22]][19:-1])\n",
    "#                 print(match_dist.at[element,'Percentage Match'])\n",
    "                tmp = pd.DataFrame(data=[[phylogeny[match_dist.at[element,'analysis/Consensus/'+path[53:-22]][19:-1]],None,match_dist.at[element,'Percentage Match']]],columns=['Species Name','Genetic Distance','Crossmapping Fraction'])\n",
    "                template_frame = template_frame.append(tmp,ignore_index=True)\n",
    "            template_frame = template_frame.set_index('Species Name')\n",
    "            for element in template_frame.index:\n",
    "                if np.isnan(large_frame.at[element,phylogeny[path[53:-23]]]):\n",
    "                    template_frame.at[element,'Genetic Distance'] = 0\n",
    "                else:\n",
    "                    template_frame.at[element,'Genetic Distance'] = 100-large_frame.at[element,phylogeny[path[53:-23]]]\n",
    "            plt.figure()\n",
    "            figures = sns.scatterplot(x=template_frame['Genetic Distance'],y=template_frame['Crossmapping Fraction'])\n",
    "            figures.set_title(\"Crossmapping Fraction vs Genetic Distance for \" + phylogeny[path[53:-23]], fontsize=15)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw line with nearest neighbour\n",
    "\n",
    "\n",
    "Draw in facet plot based on nearest neighbour?\n",
    "\n",
    "\n",
    "Color-code dots by genus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../generate_summary_dataframe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../generate_summary_dataframe.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Enter the analysis directory to generate a dataframe that extracts the details\n",
    "for each sample\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_directory\", help=\"Run from honours folder and input analysis/\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "phylogeny = {'20171103_FAH15473/barcode01': 'k__Fungi;p__Basidiomycota;c__Pucciniomycetes;o__Pucciniales;f__Pucciniaceae;g__Puccinia;s__striiformis-tritici',\n",
    "             '20171103_FAH15473/barcode02': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Capnodiales;f__Mycosphaerellaceae;g__Zymoseptoria;s__tritici',\n",
    "             '20171103_FAH15473/barcode03': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Pleosporaceae;g__Pyrenophora;s__tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Tuberaceae;g__Tuber;s__brumale',\n",
    "             '20171103_FAH15473/barcode06': 'k__Fungi;p__Basidiomycota;c__Agaricomyctes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__niger',\n",
    "             '20171103_FAH15473/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Clavispora;s__lusitaniae',\n",
    "        '20171103_FAH15473/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__unidentified', # formerly Cryptococcus neoformans, matched in both UNITE and tree to Kluyveromyces\n",
    "             '20171103_FAH15473/barcode10': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Penicillium;s__chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'k__Fungi;p__Basidiomycota;c__Microbotryomycetes;o__Sporidiobolales;f__Sporidiobolaceae;g__Rhodotorula;s__mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Microascales;f__Microascaceae;g__Scedosporium;s__boydii',\n",
    "             '20171207_FAH18654/barcode01': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Blastobotrys;s__proliferans', # matches Sugiyamella novakii in UNITE\n",
    "             '20171207_FAH18654/barcode02': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Debaryomyces;s__unidentified', # formerly Candida zeylanoides, matched in both UNITE and tree as Debaryomces/Meyerozyma\n",
    "        '20171207_FAH18654/barcode03': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Galactomyces;s__geotrichum', # tree puts it further away from other samples of its type, UNITE calls it Dipodascus\n",
    "                                     # May have larger issues with Geotrichum candidum - maybe leave out?\n",
    "             '20171207_FAH18654/barcode04': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Kodamaea;s__ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Meyerozyma;s__guilliermondii',\n",
    "             '20171207_FAH18654/barcode06': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Phaffomycetaceae;g__Wickerhamomyces;s__anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Yarrowia;s__lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Zygoascus;s__hellenicus', # matches Sugiyamella novakii in UNITE\n",
    "             '20171207_FAH18654/barcode12': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__flavus',\n",
    "             \n",
    "             '20171212_FAH18688/barcode01': 'k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__zero',\n",
    "             '20171212_FAH18688/barcode02': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__unidentified',\n",
    "         '20171212_FAH18688/barcode03': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__unidentified',\n",
    "             '20171212_FAH18688/barcode05': 'k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__yallundae-CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__yallundae-CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Botryosphaeriales;f__Botryosphaeriaceae;g__Dothiorella;s__vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'k__Fungi;p__Basidiomycota;c__Exobasidiomycetes;o__Microstromatales;f__Quambalariaceae;g__Quambalaria;s__cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Xylariales;f__Xylariaceae;g__Entoleuca;s__unidentified',\n",
    "                 '20171212_FAH18688/barcode11': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__CCL060', # may be asteroma or Diaporthe\n",
    "                 '20171212_FAH18688/barcode12': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Saccharomyces;s__cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Chaetothyriales;f__Herpotrichiellaceae;g__Cladophialophora;s__unidentified',\n",
    "             '20180108_FAH18647/barcode03': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__albicans',\n",
    "             '20180108_FAH18647/barcode04': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__parapsilosis',\n",
    "#          '20180108_FAH18647/barcode07': 'k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__gattii', # not including\n",
    "#              '20180108_FAH18647/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Geotrichum;s__candidum', # not including\n",
    "             '20180108_FAH18647/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__unidentified', # formerly Kluyveromyces lactis\n",
    "             '20180108_FAH18647/barcode10': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__membranifaciens'}\n",
    "\n",
    "summary = pd.DataFrame(data=None, columns = ['species','genus','family','order','class','phylum','kingdom','# raw reads','# reads after homology filtering','# reads after length filtering','path to raw reads','path to homology filtering','path to length filtering'])\n",
    "import glob\n",
    "path = \"/media/MassStorage/tmp/TE/honours/analysis/Consensus/*/*/\"\n",
    "path_names = glob.glob(path)\n",
    "for path in path_names:\n",
    "    if path[-13:-1] != 'unclassified' and path[-28:-1] != '20171207_FAH18654/barcode10' and path[-28:-1] != '20171212_FAH18688/barcode10' and path[-28:-1] != '20180108_FAH18647/barcode07' and path[-28:-1] != '20180108_FAH18647/barcode08':\n",
    "        key = path[-28:-1]\n",
    "        if args.verbose:\n",
    "            print('\\033[0;34m' + \"Opened barcode \" + '\\033[0;35m' + key + '\\033[1;37m')\n",
    "        species_ = phylogeny[key].split(';')[6].split('__')[1].lower()\n",
    "        genus_ = phylogeny[key].split(';')[5].split('__')[1].lower()\n",
    "        family_ = phylogeny[key].split(';')[4].split('__')[1].lower()\n",
    "        order_ = phylogeny[key].split(';')[3].split('__')[1].lower()\n",
    "        class_ = phylogeny[key].split(';')[2].split('__')[1].lower()\n",
    "        phylum_ = phylogeny[key].split(';')[1].split('__')[1].lower()\n",
    "        kingdom_ = phylogeny[key].split(';')[0].split('__')[1].lower()\n",
    "        \n",
    "        if args.verbose:\n",
    "            print('\\033[1;36m' + \"BEGIN RAW\"'\\033[1;37m')\n",
    "        raw_path = \"analysis/Concatenated/\"+key+\"/merged.fastq\"\n",
    "        raw_reads = SeqIO.to_dict(SeqIO.parse(raw_path, \"fastq\"))\n",
    "        raw_count = 0\n",
    "        for entry in raw_reads:\n",
    "            raw_count += 1\n",
    "            \n",
    "        if args.verbose:\n",
    "            print('\\033[1;36m' + \"BEGIN HOMOLOGY\"+ '\\033[1;37m')\n",
    "        homology_path = \"analysis/Python_Processing/\"+key+\"/combined_test.paf\"\n",
    "        homology_paf = pd.read_csv(homology_path, sep='\\t', header=None, engine='python')\n",
    "        homology_reads = {}\n",
    "        homology_count = 0\n",
    "        for entry in homology_paf[0].unique():\n",
    "            homology_count += 1\n",
    "            \n",
    "        if args.verbose:\n",
    "            print('\\033[1;36m' + \"BEGIN LENGTH\" + '\\033[1;37m')\n",
    "        length_path = \"analysis/Length_Filtered/\"+key+\"/length_restricted_reads.fasta\"\n",
    "        length_reads = SeqIO.to_dict(SeqIO.parse(length_path, \"fasta\"))\n",
    "        length_count = 0\n",
    "        for entry in length_reads:\n",
    "            length_count += 1\n",
    "            \n",
    "        if args.verbose:\n",
    "            print('\\033[1;36m' + \"BEGIN USE\" + '\\033[1;37m')\n",
    "        use_path = \"analysis/Length_Filtered/\"+key+\"/length_restricted_for_use.fasta\"\n",
    "        use_reads = SeqIO.to_dict(SeqIO.parse(use_path, \"fasta\"))\n",
    "        use_count = 0\n",
    "        for entry in use_reads:\n",
    "            use_count += 1\n",
    "\n",
    "\n",
    "        add = pd.DataFrame([[species_, genus_, family_, order_, class_, phylum_, kingdom_, raw_count, homology_count, length_count, use_count, raw_path, homology_path, length_path, use_path]], columns = ['species','genus','family','order','class','phylum','kingdom','# raw reads','# reads after homology filtering','# reads after length filtering','# for use', 'path to raw reads','path to homology filtering','path to length filtering', 'path for use'], index=[key])\n",
    "        summary = summary.append([add])\n",
    "summary = summary.sort_index(axis=0)\n",
    "summary = summary[['species','genus','family','order','class','phylum','kingdom','# raw reads','# reads after homology filtering','# reads after length filtering','# for use', 'path to raw reads','path to homology filtering','path to length filtering', 'path for use']]\n",
    "summary.to_csv(\"analysis/Stats/reference_dataframe.csv\")\n",
    "if args.verbose:\n",
    "            print('\\033[0;34m' + \"Reference Dataframe saved to \" + '\\033[0;35m' + \"analysis/Stats/reference_dataframe.csv\" + '\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k__Fungi;p__Basidiomycota;c__Pucciniomycetes;o__Pucciniales;f__Pucciniaceae;g__Puccinia;s__striiformis-tritici\n",
      "k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Capnodiales;f__Mycosphaerellaceae;g__Zymoseptoria;s__tritici\n",
      "k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Pleosporaceae;g__Pyrenophora;s__tritici-repentis\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__oxysporum\n",
      "k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Tuberaceae;g__Tuber;s__brumale\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomyctes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__globuliformis\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__niger\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Clavispora;s__lusitaniae\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Penicillium;s__chrysogenum\n",
      "k__Fungi;p__Basidiomycota;c__Microbotryomycetes;o__Sporidiobolales;f__Sporidiobolaceae;g__Rhodotorula;s__mucilaginosa\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Microascales;f__Microascaceae;g__Scedosporium;s__boydii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Blastobotrys;s__proliferans\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Debaryomyces;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Galactomyces;s__geotrichum\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Kodamaea;s__ohmeri\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Meyerozyma;s__guilliermondii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Phaffomycetaceae;g__Wickerhamomyces;s__anomalus\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__mexicana\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__scolyti\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Yarrowia;s__lipolytica\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Zygoascus;s__hellenicus\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__flavus\n",
      "k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__zero\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__CCL067\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__yallundae-CCL031\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__yallundae-CCL029\n",
      "k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Botryosphaeriales;f__Botryosphaeriaceae;g__Dothiorella;s__vidmadera\n",
      "k__Fungi;p__Basidiomycota;c__Exobasidiomycetes;o__Microstromatales;f__Quambalariaceae;g__Quambalaria;s__cyanescens\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Xylariales;f__Xylariaceae;g__Entoleuca;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__CCL060\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__CCL068\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Saccharomyces;s__cerevisiae\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Chaetothyriales;f__Herpotrichiellaceae;g__Cladophialophora;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__albicans\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__metapsilosis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__orthopsilosis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__parapsilosis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__marxianus\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__kudriavzevii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__membranifaciens\n"
     ]
    }
   ],
   "source": [
    "phylogeny = {'20171103_FAH15473/barcode01': 'k__Fungi;p__Basidiomycota;c__Pucciniomycetes;o__Pucciniales;f__Pucciniaceae;g__Puccinia;s__striiformis-tritici',\n",
    "             '20171103_FAH15473/barcode02': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Capnodiales;f__Mycosphaerellaceae;g__Zymoseptoria;s__tritici',\n",
    "             '20171103_FAH15473/barcode03': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Pleosporaceae;g__Pyrenophora;s__tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Tuberaceae;g__Tuber;s__brumale',\n",
    "             '20171103_FAH15473/barcode06': 'k__Fungi;p__Basidiomycota;c__Agaricomyctes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__niger',\n",
    "             '20171103_FAH15473/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Clavispora;s__lusitaniae',\n",
    "        '20171103_FAH15473/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__unidentified', # formerly Cryptococcus neoformans, matched in both UNITE and tree to Kluyveromyces\n",
    "             '20171103_FAH15473/barcode10': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Penicillium;s__chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'k__Fungi;p__Basidiomycota;c__Microbotryomycetes;o__Sporidiobolales;f__Sporidiobolaceae;g__Rhodotorula;s__mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Microascales;f__Microascaceae;g__Scedosporium;s__boydii',\n",
    "             '20171207_FAH18654/barcode01': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Blastobotrys;s__proliferans', # matches Sugiyamella novakii in UNITE\n",
    "             '20171207_FAH18654/barcode02': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Debaryomyces;s__unidentified', # formerly Candida zeylanoides, matched in both UNITE and tree as Debaryomces/Meyerozyma\n",
    "        '20171207_FAH18654/barcode03': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Galactomyces;s__geotrichum', # tree puts it further away from other samples of its type, UNITE calls it Dipodascus\n",
    "                                     # May have larger issues with Geotrichum candidum - maybe leave out?\n",
    "             '20171207_FAH18654/barcode04': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Kodamaea;s__ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Meyerozyma;s__guilliermondii',\n",
    "             '20171207_FAH18654/barcode06': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Phaffomycetaceae;g__Wickerhamomyces;s__anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Yarrowia;s__lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Zygoascus;s__hellenicus', # matches Sugiyamella novakii in UNITE\n",
    "             '20171207_FAH18654/barcode12': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__flavus',\n",
    "             \n",
    "             '20171212_FAH18688/barcode01': 'k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__zero',\n",
    "             '20171212_FAH18688/barcode02': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__unidentified',\n",
    "         '20171212_FAH18688/barcode03': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__unidentified',\n",
    "             '20171212_FAH18688/barcode05': 'k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__yallundae-CCL031',\n",
    "             '20171212_FAH18688/barcode06': 'k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__yallundae-CCL029',\n",
    "             '20171212_FAH18688/barcode07': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Botryosphaeriales;f__Botryosphaeriaceae;g__Dothiorella;s__vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'k__Fungi;p__Basidiomycota;c__Exobasidiomycetes;o__Microstromatales;f__Quambalariaceae;g__Quambalaria;s__cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Xylariales;f__Xylariaceae;g__Entoleuca;s__unidentified',\n",
    "                 '20171212_FAH18688/barcode11': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__CCL060', # may be asteroma or Diaporthe\n",
    "                 '20171212_FAH18688/barcode12': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Saccharomyces;s__cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Chaetothyriales;f__Herpotrichiellaceae;g__Cladophialophora;s__unidentified',\n",
    "             '20180108_FAH18647/barcode03': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__albicans',\n",
    "             '20180108_FAH18647/barcode04': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__parapsilosis',\n",
    "#          '20180108_FAH18647/barcode07': 'k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__gattii', # not including\n",
    "#              '20180108_FAH18647/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Geotrichum;s__candidum', # not including\n",
    "             '20180108_FAH18647/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Candida;s__unidentified', # formerly Kluyveromyces lactis\n",
    "             '20180108_FAH18647/barcode10': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__membranifaciens'}\n",
    "with open('../../analysis/Stats/taxonomy_file.csv', 'w+') as f:\n",
    "    for key in phylogeny.keys():\n",
    "        print(phylogeny[key])\n",
    "        f.write(\"%s,%s\\n\"%(key,phylogeny[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k__Fungi;p__Basidiomycota;c__Pucciniomycetes;o__Pucciniales;f__Pucciniaceae;g__Puccinia;s__Puccinia_striiformis\n",
      "k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Capnodiales;f__Mycosphaerellaceae;g__Zymoseptoria;s__Zymoseptoria_tritici\n",
      "k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Pleosporaceae;g__Pyrenophora;s__Pyrenophora_tritici-repentis\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_oxysporum\n",
      "k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Tuberaceae;g__Tuber;s__Tuber_brumale\n",
      "k__Fungi;p__Basidiomycota;c__Agaricomyctes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_globuliformis\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__Aspergillus_niger\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Clavispora;s__Clavispora_lusitaniae\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Penicillium;s__Penicillium_chrysogenum\n",
      "k__Fungi;p__Basidiomycota;c__Microbotryomycetes;o__Sporidiobolales;f__Sporidiobolaceae;g__Rhodotorula;s__Rhodotorula_mucilaginosa\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Microascales;f__Microascaceae;g__Scedosporium;s__Scedosporium_boydii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Blastobotrys;s__Blastobotrys_proliferans\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Debaryomyces;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Galactomyces;s__Galactomyces_geotrichum\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Kodamaea;s__Kodamaea_ohmeri\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Meyerozyma;s__Meyerozyma_guilliermondii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Phaffomycetaceae;g__Wickerhamomyces;s__Wickerhamomyces_anomalus\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__Yamadazyma_mexicana\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__Yamadazyma_scolyti\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Yarrowia;s__Yarrowia_lipolytica\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Zygoascus;s__Zygoascus_hellenicus\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__Aspergillus_flavus\n",
      "k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__Cryptococcus_zero\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__Diaporthe_CCL067\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__Oculimacula_yallundae\n",
      "k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__Oculimacula_yallundae\n",
      "k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Botryosphaeriales;f__Botryosphaeriaceae;g__Dothiorella;s__Dothiorella_vidmadera\n",
      "k__Fungi;p__Basidiomycota;c__Exobasidiomycetes;o__Microstromatales;f__Quambalariaceae;g__Quambalaria;s__Quambalaria_cyanescens\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Xylariales;f__Xylariaceae;g__Entoleuca;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__Asteroma_CCL060\n",
      "k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__Asteroma_CCL068\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Saccharomyces;s__Saccharomyces_cerevisiae\n",
      "k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Chaetothyriales;f__Herpotrichiellaceae;g__Cladophialophora;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_albicans\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_metapsilosis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_orthopsilosis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_parapsilosis\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__unidentified\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__Kluyveromyces_marxianus\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__Pichia_kudriavzevii\n",
      "k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__Pichia_membranifaciens\n"
     ]
    }
   ],
   "source": [
    "phylogeny = {'20171103_FAH15473/barcode01': 'k__Fungi;p__Basidiomycota;c__Pucciniomycetes;o__Pucciniales;f__Pucciniaceae;g__Puccinia;s__Puccinia_striiformis',\n",
    "             '20171103_FAH15473/barcode02': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Capnodiales;f__Mycosphaerellaceae;g__Zymoseptoria;s__Zymoseptoria_tritici',\n",
    "             '20171103_FAH15473/barcode03': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Pleosporales;f__Pleosporaceae;g__Pyrenophora;s__Pyrenophora_tritici-repentis',\n",
    "             '20171103_FAH15473/barcode04': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Hypocreales;f__Nectriaceae;g__Fusarium;s__Fusarium_oxysporum',\n",
    "             '20171103_FAH15473/barcode05': 'k__Fungi;p__Ascomycota;c__Pezizomycetes;o__Pezizales;f__Tuberaceae;g__Tuber;s__Tuber_brumale',\n",
    "             '20171103_FAH15473/barcode06': 'k__Fungi;p__Basidiomycota;c__Agaricomyctes;o__Agaricales;f__Cortinariaceae;g__Cortinarius;s__Cortinarius_globuliformis',\n",
    "             '20171103_FAH15473/barcode07': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__Aspergillus_niger',\n",
    "             '20171103_FAH15473/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Clavispora;s__Clavispora_lusitaniae',\n",
    "        '20171103_FAH15473/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__unidentified', # formerly Cryptococcus neoformans, matched in both UNITE and tree to Kluyveromyces\n",
    "             '20171103_FAH15473/barcode10': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Penicillium;s__Penicillium_chrysogenum',\n",
    "             '20171103_FAH15473/barcode11': 'k__Fungi;p__Basidiomycota;c__Microbotryomycetes;o__Sporidiobolales;f__Sporidiobolaceae;g__Rhodotorula;s__Rhodotorula_mucilaginosa',\n",
    "             '20171103_FAH15473/barcode12': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Microascales;f__Microascaceae;g__Scedosporium;s__Scedosporium_boydii',\n",
    "             '20171207_FAH18654/barcode01': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Blastobotrys;s__Blastobotrys_proliferans', # matches Sugiyamella novakii in UNITE\n",
    "             '20171207_FAH18654/barcode02': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Debaryomyces;s__unidentified', # formerly Candida zeylanoides, matched in both UNITE and tree as Debaryomces/Meyerozyma\n",
    "        '20171207_FAH18654/barcode03': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Galactomyces;s__Galactomyces_geotrichum', # tree puts it further away from other samples of its type, UNITE calls it Dipodascus\n",
    "                                     # May have larger issues with Geotrichum candidum - maybe leave out?\n",
    "             '20171207_FAH18654/barcode04': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Metschnikowiaceae;g__Kodamaea;s__Kodamaea_ohmeri',\n",
    "             '20171207_FAH18654/barcode05': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Meyerozyma;s__Meyerozyma_guilliermondii',\n",
    "             '20171207_FAH18654/barcode06': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Phaffomycetaceae;g__Wickerhamomyces;s__Wickerhamomyces_anomalus',\n",
    "             '20171207_FAH18654/barcode07': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__Yamadazyma_mexicana',\n",
    "             '20171207_FAH18654/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Debaryomycetaceae;g__Yamadazyma;s__Yamadazyma_scolyti',\n",
    "             '20171207_FAH18654/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Yarrowia;s__Yarrowia_lipolytica',\n",
    "             '20171207_FAH18654/barcode11': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Trichomonascaceae;g__Zygoascus;s__Zygoascus_hellenicus', # matches Sugiyamella novakii in UNITE\n",
    "             '20171207_FAH18654/barcode12': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__Aspergillus_flavus',\n",
    "             \n",
    "             '20171212_FAH18688/barcode01': 'k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__Cryptococcus_zero',\n",
    "             '20171212_FAH18688/barcode02': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Eurotiales;f__Aspergillaceae;g__Aspergillus;s__unidentified',\n",
    "         '20171212_FAH18688/barcode03': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__Diaporthe_CCL067',\n",
    "             '20171212_FAH18688/barcode04': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Diaporthaceae;g__Diaporthe;s__unidentified',\n",
    "             '20171212_FAH18688/barcode05': 'k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__Oculimacula_yallundae',\n",
    "             '20171212_FAH18688/barcode06': 'k__Fungi;p__Ascomycota;c__Leotiomycetes;o__Helotiales;f__Dermateaceae;g__Oculimacula;s__Oculimacula_yallundae',\n",
    "             '20171212_FAH18688/barcode07': 'k__Fungi;p__Ascomycota;c__Dothideomycetes;o__Botryosphaeriales;f__Botryosphaeriaceae;g__Dothiorella;s__Dothiorella_vidmadera',\n",
    "             '20171212_FAH18688/barcode08': 'k__Fungi;p__Basidiomycota;c__Exobasidiomycetes;o__Microstromatales;f__Quambalariaceae;g__Quambalaria;s__Quambalaria_cyanescens',\n",
    "             '20171212_FAH18688/barcode09': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Xylariales;f__Xylariaceae;g__Entoleuca;s__unidentified',\n",
    "                 '20171212_FAH18688/barcode11': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__Asteroma_CCL060', # may be asteroma or Diaporthe\n",
    "                 '20171212_FAH18688/barcode12': 'k__Fungi;p__Ascomycota;c__Sordariomycetes;o__Diaporthales;f__Gnomoniaceae;g__Asteroma;s__Asteroma_CCL068',\n",
    "             '20180108_FAH18647/barcode01': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Saccharomyces;s__Saccharomyces_cerevisiae',\n",
    "             '20180108_FAH18647/barcode02': 'k__Fungi;p__Ascomycota;c__Eurotiomycetes;o__Chaetothyriales;f__Herpotrichiellaceae;g__Cladophialophora;s__unidentified',\n",
    "             '20180108_FAH18647/barcode03': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_albicans',\n",
    "             '20180108_FAH18647/barcode04': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_metapsilosis',\n",
    "             '20180108_FAH18647/barcode05': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_orthopsilosis',\n",
    "             '20180108_FAH18647/barcode06': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__Candida_parapsilosis',\n",
    "#          '20180108_FAH18647/barcode07': 'k__Fungi;p__Basidiomycota;c__Tremellomycetes;o__Tremellales;f__Tremellaceae;g__Cryptococcus;s__gattii', # not including\n",
    "#              '20180108_FAH18647/barcode08': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Dipodascaceae;g__Geotrichum;s__candidum', # not including\n",
    "             '20180108_FAH18647/barcode09': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetales_fam_Incertae_sedis;g__Candida;s__unidentified', # formerly Kluyveromyces lactis\n",
    "             '20180108_FAH18647/barcode10': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Saccharomycetaceae;g__Kluyveromyces;s__Kluyveromyces_marxianus',\n",
    "             '20180108_FAH18647/barcode11': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__Pichia_kudriavzevii',\n",
    "             '20180108_FAH18647/barcode12': 'k__Fungi;p__Ascomycota;c__Saccharomycetes;o__Saccharomycetales;f__Pichiaceae;g__Pichia;s__Pichia_membranifaciens'}\n",
    "with open('../../analysis/Stats/taxonomy_file_qiime.csv', 'w+') as f:\n",
    "    for key in phylogeny.keys():\n",
    "        print(phylogeny[key])\n",
    "        f.write(\"%s,%s\\n\"%(key,phylogeny[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../numberfy_fasta.py\n",
    "\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "# import seaborn as sns\n",
    "import csv\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "from shutil import copy\n",
    "import random\n",
    "import warnings\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "# from ipysankeywidget import SankeyWidget\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "This creates a machine-learning friendly version of each length restricted fasta file\n",
    "\"\"\")\n",
    "group = parser.add_mutually_exclusive_group()\n",
    "group.add_argument(\"--verbose\", \"-v\", \"--v\", action=\"store_true\")\n",
    "group.add_argument(\"--quiet\", \"-q\", \"--q\", action=\"store_true\")\n",
    "parser.add_argument(\"input_file\", help=\"The input file for extraction\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.verbose:\n",
    "    print(\"Input file is \" + args.input_file + \"\\n\")\n",
    "\n",
    "fasta = SeqIO.to_dict(SeqIO.parse(args.input_file, \"fasta\"))\n",
    "\n",
    "fasta_numbers = fasta.copy()\n",
    "\n",
    "for key in fasta:\n",
    "    seq = str(fasta[key].seq[30:-30]).replace(\"A\",'0').replace(\"C\",'1').replace(\"G\",'2').replace(\"T\",'3')\n",
    "#     if len(seq) < max(total_lens):\n",
    "#         seq = seq + '4'*(max(total_lens)-len(seq))\n",
    "    fasta_numbers[key].seq = Seq(seq)\n",
    "\n",
    "with open(args.input_file[:-6]+'_numbers.fasta', \"w\") as output_handle:\n",
    "    SeqIO.write(fasta_numbers.values(), output_handle, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WILL NEED TO APPLY FILTERING TO BADREAD GENERATED READS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord as SR\n",
    "from Bio.Blast import NCBIXML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "# Load the full file containing all reads for this barcode\n",
    "full_file_dict = SeqIO.to_dict(SeqIO.parse('../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_extended.fastq', \"fastq\"))\n",
    "\n",
    "# Extract the information about the lengths of the sequence for each read in this barcode\n",
    "full_lengths = []\n",
    "for key in full_file_dict:\n",
    "    if 1500 < len(full_file_dict[key].seq) < 5000:\n",
    "        full_lengths.append(len(full_file_dict[key].seq))\n",
    "full_lengths_len = len(full_lengths)\n",
    "\n",
    "ax = sns.distplot(full_lengths, color=\"k\", kde=False, bins=5000)\n",
    "ax.set(xlim=(250, 3500))\n",
    "ax.set_title(\"Read spread for extended\", fontsize=15)\n",
    "ax.set_xlabel(\"Length of read\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "figure1 = ax.get_figure()\n",
    "display(figure1)\n",
    "figure1.clf()\n",
    "\n",
    "mean = np.mean(full_lengths)\n",
    "std = np.std(full_lengths)\n",
    "\n",
    "print(mean)\n",
    "print(std)\n",
    "    \n",
    "print(full_lengths_len)\n",
    "    \n",
    "length_filt_dict = full_file_dict.copy()\n",
    "for key in full_file_dict:\n",
    "    if len(full_file_dict[key].seq) < (mean-1.645*std) or len(full_file_dict[key].seq) > (mean+1.645*std):\n",
    "        del length_filt_dict[key]\n",
    "print(len(length_filt_dict))\n",
    "\n",
    "        \n",
    "        \n",
    "SeqIO.write(length_filt_dict.values(), '../../analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_extended.fasta', \"fasta\")\n",
    "# if args.verbose:\n",
    "#     print('\\033[1;36m' + 'Saved %s' % ('/'.join([output_folder, 'length_restricted_reads.fasta'])) + '\\033[1;37m')     \n",
    "    \n",
    "    \n",
    "    \n",
    "# length_filt_lens = []\n",
    "# len_filt_keys = []\n",
    "# for key in length_filt_dict:\n",
    "#     length_filt_lens.append(len(length_filt_dict[key].seq))\n",
    "#     len_filt_keys.append(key)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Extract the qscores\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;34m' + \"Loading \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "# summ_stats_csv = pd.read_csv('Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt', sep='\\t', header=None, names=['filename', 'read_id', 'run_id', 'batch_id', 'channel', 'mux', 'start_time', 'duration', 'num_events', 'passes_filtering', 'template_start', 'num_events_template', 'template_duration', 'sequence_length_template', 'mean_qscore_template', 'strand_score_template', 'median_template', 'mad_template'], engine='python')\n",
    "# summ_stats_csv = pd.DataFrame(summ_stats_csv[1:])\n",
    "# summary_list = []\n",
    "# for column, row in summ_stats_csv.iterrows():\n",
    "#     if row['read_id'] in full_keys:\n",
    "#         summary_list.append([row['read_id'], row['mean_qscore_template']])\n",
    "# summary_frame = pd.DataFrame(summary_list)\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;34m' + \"Finished with \" + 'Basecalled/'+'/'.join(args.full_file.rsplit('/')[-3:-1])+'/sequencing_summary.txt' + '\\033[1;37m')\n",
    "    \n",
    "# Create a dictionary containing the statistics for the filtered dataset\n",
    "#     Total no. frDNA reads, Min. read length, Max. read length, Mean read length, Median read length, Quality score\n",
    "\n",
    "# stats_dict = {'number of frDNA reads':len(length_filt_lens),'minimum read length':min(length_filt_lens),'maximum read length':max(length_filt_lens),'mean read length':\"{:.0f}\".format(np.mean(length_filt_lens)),'std dev':\"{:.0f}\".format(np.std(length_filt_lens)),'median read length':\"{:.0f}\".format(np.median(length_filt_lens))\n",
    "#               ,'min_qscore':\"{:.2f}\".format(min(summary_frame[1].astype(float))), 'max_qscore':\"{:.2f}\".format(max(summary_frame[1].astype(float))), 'mean_qscore':\"{:.2f}\".format(np.mean(summary_frame[1].astype(float))), 'median_qscore':\"{:.2f}\".format(np.median(summary_frame[1].astype(float)))\n",
    "#              }\n",
    "# stats = pd.DataFrame(stats_dict, index=['%s' % '/'.join(args.full_file.rsplit('/')[-3:-1])])    \n",
    "              \n",
    "# bx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "# bx.set(xlim=(250, 3500))\n",
    "# bx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "# bx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# bx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# figure2 = bx.get_figure()\n",
    "# figure2.savefig('/'.join([output_folder, 'frDNA_len_filt_full.png']))\n",
    "# figure2.clf()\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;32m' + \"frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_full.png']) + '\\033[1;37m')\n",
    "\n",
    "# cx = sns.distplot(length_filt_lens, color=\"k\", kde=False)\n",
    "# cx.set(xlim=((mean-1.645*std)-100, (mean+1.645*std)+100))\n",
    "# cx.set_title(\"frDNA reads for %s\" % '/'.join(args.full_file.rsplit('/')[-3:-1]), fontsize=15)\n",
    "# cx.set_xlabel(\"Length of read\", fontsize=13)\n",
    "# cx.set_ylabel(\"Number of reads\", fontsize=13)\n",
    "# figure3 = cx.get_figure()\n",
    "# figure3.savefig('/'.join([output_folder, 'frDNA_len_filt_limited.png']))\n",
    "# figure3.clf()\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;32m' + \"Zoomed-in frDNA spread image file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_limited.png']) + '\\033[1;37m')\n",
    "\n",
    "# stats.to_csv('/'.join([output_folder, 'frDNA_len_filt_statistics.csv']), index=False)\n",
    "# if args.verbose:\n",
    "#     print('\\033[0;32m' + \"Summary statistics file saved to \" + '/'.join([output_folder, 'frDNA_len_filt_statistics.csv']) + '\\033[1;37m')\n",
    "    \n",
    "# print('\\033[0;35m'+'END'+'\\033[1;37m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_dict = {\n",
    "    1: 'kingdom fungi',\n",
    "              2: 'phylum ascomycota',\n",
    "              3: 'class eurotiomycetes',\n",
    "              4: 'family aspergillaceae',\n",
    "              5: 'genus aspergillus',\n",
    "              6: 'class dothideomycetes',\n",
    "              7: 'genus oculimacula',\n",
    "              8: 'order saccharomycetales',\n",
    "              9: 'family debaryomycetaceae',\n",
    "              10: 'genus yamadazyma',\n",
    "              11: 'family dipodascaceae',\n",
    "              12: 'family metschnikowiaceae',\n",
    "              13: 'genus pichia',\n",
    "              14: 'family saccharomycetaceae',\n",
    "              15: 'genus candida',\n",
    "              16: 'genus kluyveromyces',\n",
    "              17: 'family trichomonascaceae',\n",
    "              18: 'class sordariomycetes',\n",
    "              19: 'order diaporthales',\n",
    "              20: 'genus diaporthe',\n",
    "              21: 'genus asteroma',\n",
    "              22: 'phylum basidiomycota'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kingdom fungi\n",
      "phylum ascomycota\n",
      "class eurotiomycetes\n",
      "family trichocomaceae\n",
      "genus aspergillus\n",
      "class dothideomycetes\n",
      "genus oculimacula\n",
      "order saccharomycetales\n",
      "family debaryomycetaceae\n",
      "genus yamadazyma\n",
      "family dipodascaceae\n",
      "family metschnikowiaceae\n",
      "genus pichia\n",
      "family saccharomycetaceae\n",
      "genus candida\n",
      "genus kluyveromyces\n",
      "family trichomonascaceae\n",
      "class sordariomycetes\n",
      "order diaporthales\n",
      "genus diaporthe\n",
      "genus asteroma\n",
      "phylum basidiomycota\n"
     ]
    }
   ],
   "source": [
    "for key in nodes_dict:\n",
    "    print(nodes_dict[key].split(' ')[0], nodes_dict[key].split(' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kingdom fungi\n",
      "phylum ascomycota\n",
      "class eurotiomycetes\n",
      "family trichocomaceae\n",
      "genus aspergillus\n",
      "class dothideomycetes\n",
      "genus oculimacula\n",
      "order saccharomycetales\n",
      "family debaryomycetaceae\n",
      "genus yamadazyma\n",
      "family dipodascaceae\n",
      "family metschnikowiaceae\n",
      "genus pichia\n",
      "family saccharomycetaceae\n",
      "genus candida\n",
      "genus kluyveromyces\n",
      "family trichomonascaceae\n",
      "class sordariomycetes\n",
      "order diaporthales\n",
      "genus diaporthe\n",
      "genus asteroma\n",
      "phylum basidiomycota\n"
     ]
    }
   ],
   "source": [
    "with open('../../analysis/Stats/nodes.csv', 'w+') as f:\n",
    "    for key in nodes_dict.keys():\n",
    "        print(nodes_dict[key])\n",
    "        f.write(\"%s\\n\"%(nodes_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = pd.read_csv('../../analysis/Stats/reference_dataframe.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis/Length_Filtered/20171103_FAH15473/barcode01/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode02/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode03/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode04/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode05/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode06/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode07/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode08/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode09/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode10/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode11/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171103_FAH15473/barcode12/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode01/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode02/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode03/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode04/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode05/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode06/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode07/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode08/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode09/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode11/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171207_FAH18654/barcode12/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode01/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode02/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode03/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode04/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode05/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode06/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode07/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode08/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode09/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode11/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20171212_FAH18688/barcode12/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode01/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode02/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode03/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode04/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode05/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode06/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode09/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode10/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode11/length_restricted_for_use.fasta\n",
      "\n",
      "\n",
      "analysis/Length_Filtered/20180108_FAH18647/barcode12/length_restricted_for_use.fasta\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in ref_df.iterrows():\n",
    "    print(row['path for use'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
